{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solutions Delivery Platform \u00b6 Welcome! Concepts Concept pages are understanding oriented articles describing how SDP works. Libraries Library pages explain the available SDP Pipeline Libraries. Tutorials Tutorials are learning oriented lessons to teach users about SDP. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems. Work In Progress This docs site is still a work in progress! Check out the contributing section if you're interested in helping.","title":"Home"},{"location":"#solutions-delivery-platform","text":"Welcome! Concepts Concept pages are understanding oriented articles describing how SDP works. Libraries Library pages explain the available SDP Pipeline Libraries. Tutorials Tutorials are learning oriented lessons to teach users about SDP. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems. Work In Progress This docs site is still a work in progress! Check out the contributing section if you're interested in helping.","title":"Solutions Delivery Platform"},{"location":"glossary/","text":"","title":"Glossary"},{"location":"concepts/overview/","text":"Overview \u00b6 Coming Soon! Concepts is next up on the priority list after this initial release of the new docs site is over!","title":"Overview"},{"location":"concepts/overview/#overview","text":"Coming Soon! Concepts is next up on the priority list after this initial release of the new docs site is over!","title":"Overview"},{"location":"concepts/unit-testing/","text":"Overview \u00b6 The purpose of writing unit tests for pipeline libraries is to confirm that the libraries functions as expected. It is also a way to confirm that any features that were added to a library didn't inadvertently break other features.","title":"Overview"},{"location":"concepts/unit-testing/#overview","text":"The purpose of writing unit tests for pipeline libraries is to confirm that the libraries functions as expected. It is also a way to confirm that any features that were added to a library didn't inadvertently break other features.","title":"Overview"},{"location":"concepts/unit-testing/executing-tests/","text":"Test Execution \u00b6 In SDP Pipeline Libraries, tests are placed in a directory called test within the library directory. Executing tests with Gradle \u00b6 This repository has been set up to run Jenkins-Spock tests using Gradle. Currently, Gradle 6.3.0 running on JDK 8 is required. These can be downloaded on OSX via: # gradle via sdkman: https://sdkman.io/ curl -s \"https://get.sdkman.io\" | bash source \" $( pwd ) /.sdkman/bin/sdkman-init.sh\" sdk install gradle 6 .3 # java8 brew install adoptopenjdk8 export JAVA_8_HOME = $( /usr/libexec/java_home -v1.8 ) alias java8 = 'export JAVA_HOME=$JAVA_8_HOME' java8 See the Contributing Guide for instructions on running unit tests.","title":"Test Execution"},{"location":"concepts/unit-testing/executing-tests/#test-execution","text":"In SDP Pipeline Libraries, tests are placed in a directory called test within the library directory.","title":"Test Execution"},{"location":"concepts/unit-testing/executing-tests/#executing-tests-with-gradle","text":"This repository has been set up to run Jenkins-Spock tests using Gradle. Currently, Gradle 6.3.0 running on JDK 8 is required. These can be downloaded on OSX via: # gradle via sdkman: https://sdkman.io/ curl -s \"https://get.sdkman.io\" | bash source \" $( pwd ) /.sdkman/bin/sdkman-init.sh\" sdk install gradle 6 .3 # java8 brew install adoptopenjdk8 export JAVA_8_HOME = $( /usr/libexec/java_home -v1.8 ) alias java8 = 'export JAVA_HOME=$JAVA_8_HOME' java8 See the Contributing Guide for instructions on running unit tests.","title":"Executing tests with Gradle"},{"location":"concepts/unit-testing/faq/","text":"Frequently Asked Questions \u00b6 This section covers questions that aren't answered in the Spock or Jenkins-Spock documentation. What's the difference between explicitlyMockPipelineStep and explicitlyMockPipelineVariable? Practically speaking, the difference is you can omit \".call\" for explicitlyMockPipelineStep() when you use getPipelineMock() In the example on the Writing Tests page, explicitlyMockPipelineStep() is used to mock error . In this case, 1 * getPipelineMock(\"error\") can be used to see if the error pipeline step is run. If explicitlyMockPipelineVariable() had been used instead, 1 * getPipelineMock(\"error.call\") could be used to check for the error pipeline step. There may be some additional differences as well, so try to use what makes the most sense. What if the exact parameters are unknown? There are ways to match parameters to regex expressions, as well as test parameters individually The standard format for interaction-based tests is: < count > * getPipelineMock (< method >)(< parameter ( s )>) While you can put the exact parameter value in the second parentheses, you can also run arbitrary groovy code inside curly brackets. If it's a \"match\" depends on if that code returns true or false . A good example is in PenetrationTestSpec.groovy . Use it to get the value of the parameter: 1 * getPipelineMock(\"sh\")({it =~ / (zap-cli open-url) Kirk (.+)/}) . Is interaction-based testing required? No, but you can't get variables the same way as traditional Spock tests This is because the script gets run within the loadPipelineScriptForTest object. You can only access the limited set of variables stored in the binding. It makes more sense to see how variables are being used in pipeline steps, and make sure those pipeline steps use the correct value for those variables. Similarly, if you need to control how a variable is set, you need to stub whatever method or pipeline step sets the initial value for that variable. As an example, in PenetrationTestSpec.groovy , the target variable in penetration_test.groovy is tested by checking the parameters to an sh step. What troubleshooting steps are required for 'can't run method foo() on null' errors? You need to find a way to stub the method that sets the value for the object that calls foo() Check out an example in GetImagesToBuildSpec.groovy .","title":"Frequently Asked Questions"},{"location":"concepts/unit-testing/faq/#frequently-asked-questions","text":"This section covers questions that aren't answered in the Spock or Jenkins-Spock documentation. What's the difference between explicitlyMockPipelineStep and explicitlyMockPipelineVariable? Practically speaking, the difference is you can omit \".call\" for explicitlyMockPipelineStep() when you use getPipelineMock() In the example on the Writing Tests page, explicitlyMockPipelineStep() is used to mock error . In this case, 1 * getPipelineMock(\"error\") can be used to see if the error pipeline step is run. If explicitlyMockPipelineVariable() had been used instead, 1 * getPipelineMock(\"error.call\") could be used to check for the error pipeline step. There may be some additional differences as well, so try to use what makes the most sense. What if the exact parameters are unknown? There are ways to match parameters to regex expressions, as well as test parameters individually The standard format for interaction-based tests is: < count > * getPipelineMock (< method >)(< parameter ( s )>) While you can put the exact parameter value in the second parentheses, you can also run arbitrary groovy code inside curly brackets. If it's a \"match\" depends on if that code returns true or false . A good example is in PenetrationTestSpec.groovy . Use it to get the value of the parameter: 1 * getPipelineMock(\"sh\")({it =~ / (zap-cli open-url) Kirk (.+)/}) . Is interaction-based testing required? No, but you can't get variables the same way as traditional Spock tests This is because the script gets run within the loadPipelineScriptForTest object. You can only access the limited set of variables stored in the binding. It makes more sense to see how variables are being used in pipeline steps, and make sure those pipeline steps use the correct value for those variables. Similarly, if you need to control how a variable is set, you need to stub whatever method or pipeline step sets the initial value for that variable. As an example, in PenetrationTestSpec.groovy , the target variable in penetration_test.groovy is tested by checking the parameters to an sh step. What troubleshooting steps are required for 'can't run method foo() on null' errors? You need to find a way to stub the method that sets the value for the object that calls foo() Check out an example in GetImagesToBuildSpec.groovy .","title":"Frequently Asked Questions"},{"location":"concepts/unit-testing/jenkins-spock/","text":"Jenkins Spock \u00b6 Pipeline libraries are tested using Jenkins-Spock , a variation of the Spock testing framework that has been designed around testing Jenkins pipelines. Writing a Specification File \u00b6 A specification is a list of features derived from business requirements. A specification file contains that list of features as unit tests, and those tests validate that the features work as expected. There should be a separate file for each pipeline step in your library. Below is an outline of a specification file. It shows what you need to include to run tests, as well as some conventions for what to name methods and variables. Create a groovy file with the same name as the class (such as MyPipelineStepSpec.groovy ) and use this outline to get started, making sure to swap names with ones for your library. Sample Specification \u00b6 Import the framework Create a class extending JTEPipelineSpecification Create a field to house the loaded step Define a setup method where you will load the step Write a test // Create a new class for the Spec // The naming convention is the pipeline step's name, followed by Spec, // all camel-cased starting w/ a capital. public class MyPipelineStepSpec extends JTEPipelineSpecification { // Define the variable that will store the step's groovy code. This variable // follows the same naming variable as the class name, with Spec omitted. def MyPipelineStep = null // setup() is a fixture method that gets run before every test. // http://spockframework.org/spock/docs/1.2/spock_primer.html#_fixture_methods def setup () { // It's required to load the pipeline script as part of setup() // With the library monorepo, pipeline step groovy files can be found in \"sdp/libraries\" MyPipelineStep = loadPipelineScriptForTest ( \"sdp/libraries/my_library/my_pipeline_step.groovy\" ) } // Write a test (i.e. Feature Method) for each feature you wish to validate // http://spockframework.org/spock/docs/1.2/spock_primer.html#_feature_methods def \"Successful Build Sends Success Result\" () { setup: // unlike in the pipeline, the config object is not loaded during // unit tests. Use this to set it manually MyPipelineStep . getBinding (). setVariable ( \"config\" , [ field: \"String\" ]) when: // This is the \"stimulus\". It does things to test what happens // Typically, you execute the step's groovy code like this MyPipelineStep () then: // Here's where you describe the expected response // If everything in here is valid, the test passes 1 * getPipelineMock ( \"sh\" )( \"echo \\\"field = String\\\"\" ) } }","title":"Jenkins Spock"},{"location":"concepts/unit-testing/jenkins-spock/#jenkins-spock","text":"Pipeline libraries are tested using Jenkins-Spock , a variation of the Spock testing framework that has been designed around testing Jenkins pipelines.","title":"Jenkins Spock"},{"location":"concepts/unit-testing/jenkins-spock/#writing-a-specification-file","text":"A specification is a list of features derived from business requirements. A specification file contains that list of features as unit tests, and those tests validate that the features work as expected. There should be a separate file for each pipeline step in your library. Below is an outline of a specification file. It shows what you need to include to run tests, as well as some conventions for what to name methods and variables. Create a groovy file with the same name as the class (such as MyPipelineStepSpec.groovy ) and use this outline to get started, making sure to swap names with ones for your library.","title":"Writing a Specification File"},{"location":"concepts/unit-testing/jenkins-spock/#sample-specification","text":"Import the framework Create a class extending JTEPipelineSpecification Create a field to house the loaded step Define a setup method where you will load the step Write a test // Create a new class for the Spec // The naming convention is the pipeline step's name, followed by Spec, // all camel-cased starting w/ a capital. public class MyPipelineStepSpec extends JTEPipelineSpecification { // Define the variable that will store the step's groovy code. This variable // follows the same naming variable as the class name, with Spec omitted. def MyPipelineStep = null // setup() is a fixture method that gets run before every test. // http://spockframework.org/spock/docs/1.2/spock_primer.html#_fixture_methods def setup () { // It's required to load the pipeline script as part of setup() // With the library monorepo, pipeline step groovy files can be found in \"sdp/libraries\" MyPipelineStep = loadPipelineScriptForTest ( \"sdp/libraries/my_library/my_pipeline_step.groovy\" ) } // Write a test (i.e. Feature Method) for each feature you wish to validate // http://spockframework.org/spock/docs/1.2/spock_primer.html#_feature_methods def \"Successful Build Sends Success Result\" () { setup: // unlike in the pipeline, the config object is not loaded during // unit tests. Use this to set it manually MyPipelineStep . getBinding (). setVariable ( \"config\" , [ field: \"String\" ]) when: // This is the \"stimulus\". It does things to test what happens // Typically, you execute the step's groovy code like this MyPipelineStep () then: // Here's where you describe the expected response // If everything in here is valid, the test passes 1 * getPipelineMock ( \"sh\" )( \"echo \\\"field = String\\\"\" ) } }","title":"Sample Specification"},{"location":"concepts/unit-testing/writing-tests/","text":"Writing Tests \u00b6 Now that you've laid the groundwork for your tests, it's time to write them. These are the \"Feature Methods\" because there should be one for each feature. Some things to write tests for are: Things are built correctly (objects, string variables, maps, etc.) Conditional Hierarchies function as expected Variables get passed correctly Things fail when they're supposed to Once you know the feature you're testing, like \"Pipeline Fails When Config Is Undefined,\" write a feature method for it: def \"Pipeline Fails When Config Is Undefined\" () { } Now create a setup: block to define some do some pre-test preparation not covered by the setup() fixture method. In this example, the binding variable config is set to null , and a mock for the error pipeline step is created. def \"Pipeline Fails When Config Is Undefined\" () { setup: explicitlyMockPipelineStep ( \"error\" ) MyPipelineStep . getBinding (). setVariable ( \"config\" , null ) } The next step is to execute the pipeline step and test the response. This happens in the when: and then: blocks, respectively. In this example, the pipeline step is called (with no parameters), and expects the error step to be called exactly once with the message \"ERROR: config is not defined\" def \"Pipeline Fails When Config Is Undefined\" () { setup: explicitlyMockPipelineStep ( \"error\" ) MyPipelineStep . getBinding (). setVariable ( \"config\" , null ) when: MyPipelineStep () // Run the pipeline step we loaded, with no parameters then: 1 * getPipelineMock ( \"error\" )( \"ERROR: config is not defined\" ) } And that's the gist of it. You can add as many feature methods as necessary in the spec file, testing a variety of things. Be sure to check out the Spock Documentation , Jenkins-Spock Documentation , and already-created spec files in the SDP Libraries repository for examples.","title":"Writing Tests"},{"location":"concepts/unit-testing/writing-tests/#writing-tests","text":"Now that you've laid the groundwork for your tests, it's time to write them. These are the \"Feature Methods\" because there should be one for each feature. Some things to write tests for are: Things are built correctly (objects, string variables, maps, etc.) Conditional Hierarchies function as expected Variables get passed correctly Things fail when they're supposed to Once you know the feature you're testing, like \"Pipeline Fails When Config Is Undefined,\" write a feature method for it: def \"Pipeline Fails When Config Is Undefined\" () { } Now create a setup: block to define some do some pre-test preparation not covered by the setup() fixture method. In this example, the binding variable config is set to null , and a mock for the error pipeline step is created. def \"Pipeline Fails When Config Is Undefined\" () { setup: explicitlyMockPipelineStep ( \"error\" ) MyPipelineStep . getBinding (). setVariable ( \"config\" , null ) } The next step is to execute the pipeline step and test the response. This happens in the when: and then: blocks, respectively. In this example, the pipeline step is called (with no parameters), and expects the error step to be called exactly once with the message \"ERROR: config is not defined\" def \"Pipeline Fails When Config Is Undefined\" () { setup: explicitlyMockPipelineStep ( \"error\" ) MyPipelineStep . getBinding (). setVariable ( \"config\" , null ) when: MyPipelineStep () // Run the pipeline step we loaded, with no parameters then: 1 * getPipelineMock ( \"error\" )( \"ERROR: config is not defined\" ) } And that's the gist of it. You can add as many feature methods as necessary in the spec file, testing a variety of things. Be sure to check out the Spock Documentation , Jenkins-Spock Documentation , and already-created spec files in the SDP Libraries repository for examples.","title":"Writing Tests"},{"location":"contributing/","text":"Contributing Guide \u00b6 Repository Structure \u00b6 Repository Component Description README.md Repository overview, displays on GitHub. docs/index.md Repository overview, gets compiled as the landing page for the documentation docs Documentation not specific to a particular library, and assets for library docs libraries The base directory where the libraries are stored Library Structure \u00b6 Within the libraries directory, there are several components to be aware of: Component Description README.md The library's documentation page steps The steps contributed by the library resources Any reusable content for consumption by the library src The classes contributed by the library test The tests for the library For example, the current repository's a11y library: libraries/a11y \u251c\u2500\u2500 README.md \u251c\u2500\u2500 resources \u251c\u2500\u2500 src \u251c\u2500\u2500 steps \u2514\u2500\u2500 accessibility_compliance_test.groovy \u251c\u2500\u2500 test \u2514\u2500\u2500 AccessibilityComplianceTestSpec.groovy Required Tools \u00b6 Tool Purpose Gradle Used to run unit tests Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview Create a Library \u00b6 Create a new library by running: just create <library name> This will produce a folder of the library name with the library structure described above stubbed out. Information for developing a new library can be found on the Create a New Library page. Documentation \u00b6 This repository uses Material for MkDocs to build the documentation. Building the Docs \u00b6 To build the documentation, run: just build This will build the documentation and produce static HTML in the site directory. Live Reloading \u00b6 To see changes live as you make them, run: just serve The documentation will be accessible at http://localhost:8000 . Testing \u00b6 Unit tests can be written using Jenkins Spock . Currently, Gradle 6.3.0 running on JDK 8 is required to run the tests. To see how to install these tools go to Executing Tests . These tests should go in the test directory for each library. To run all the tests, run: just test The Gradle test report is published to target/reports/tests/test/index.html . For a specific library \u00b6 To run tests for a specific library, docker for example, run: just test '*docker*' For a specific Specification file \u00b6 To run tests for a specific Specification file, test/docker/BuildSpec.groovy for example, run: just test \"*.BuildSpec\" Linting \u00b6 This repository uses npm-groovy-lint with the recommended CodeNarc profile for Jenkins. The .groovylintrc.json file can be used to tune the rule profile. To lint the libraries, run: just lint-code just lint-docs The output will go to standard out. Release Management \u00b6 This repository automates the creation of release branches and tags as well as publishes the documentation for each version. Release Automation \u00b6 To cut a new release, run: just release $version Which will: create a release/$version branch create a $version tag publish the documentation for the version and update the latest documentation alias Automated Changelogs \u00b6 Release Drafter is used to automate release note updates as Pull Requests are opened to main . The configuration for Release Drafter exists in the .github/release-drafter.yml file and uses GitHub Actions.","title":"Contributing Guide"},{"location":"contributing/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"contributing/#repository-structure","text":"Repository Component Description README.md Repository overview, displays on GitHub. docs/index.md Repository overview, gets compiled as the landing page for the documentation docs Documentation not specific to a particular library, and assets for library docs libraries The base directory where the libraries are stored","title":"Repository Structure"},{"location":"contributing/#library-structure","text":"Within the libraries directory, there are several components to be aware of: Component Description README.md The library's documentation page steps The steps contributed by the library resources Any reusable content for consumption by the library src The classes contributed by the library test The tests for the library For example, the current repository's a11y library: libraries/a11y \u251c\u2500\u2500 README.md \u251c\u2500\u2500 resources \u251c\u2500\u2500 src \u251c\u2500\u2500 steps \u2514\u2500\u2500 accessibility_compliance_test.groovy \u251c\u2500\u2500 test \u2514\u2500\u2500 AccessibilityComplianceTestSpec.groovy","title":"Library Structure"},{"location":"contributing/#required-tools","text":"Tool Purpose Gradle Used to run unit tests Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview","title":"Required Tools"},{"location":"contributing/#create-a-library","text":"Create a new library by running: just create <library name> This will produce a folder of the library name with the library structure described above stubbed out. Information for developing a new library can be found on the Create a New Library page.","title":"Create a Library"},{"location":"contributing/#documentation","text":"This repository uses Material for MkDocs to build the documentation.","title":"Documentation"},{"location":"contributing/#building-the-docs","text":"To build the documentation, run: just build This will build the documentation and produce static HTML in the site directory.","title":"Building the Docs"},{"location":"contributing/#live-reloading","text":"To see changes live as you make them, run: just serve The documentation will be accessible at http://localhost:8000 .","title":"Live Reloading"},{"location":"contributing/#testing","text":"Unit tests can be written using Jenkins Spock . Currently, Gradle 6.3.0 running on JDK 8 is required to run the tests. To see how to install these tools go to Executing Tests . These tests should go in the test directory for each library. To run all the tests, run: just test The Gradle test report is published to target/reports/tests/test/index.html .","title":"Testing"},{"location":"contributing/#for-a-specific-library","text":"To run tests for a specific library, docker for example, run: just test '*docker*'","title":"For a specific library"},{"location":"contributing/#for-a-specific-specification-file","text":"To run tests for a specific Specification file, test/docker/BuildSpec.groovy for example, run: just test \"*.BuildSpec\"","title":"For a specific Specification file"},{"location":"contributing/#linting","text":"This repository uses npm-groovy-lint with the recommended CodeNarc profile for Jenkins. The .groovylintrc.json file can be used to tune the rule profile. To lint the libraries, run: just lint-code just lint-docs The output will go to standard out.","title":"Linting"},{"location":"contributing/#release-management","text":"This repository automates the creation of release branches and tags as well as publishes the documentation for each version.","title":"Release Management"},{"location":"contributing/#release-automation","text":"To cut a new release, run: just release $version Which will: create a release/$version branch create a $version tag publish the documentation for the version and update the latest documentation alias","title":"Release Automation"},{"location":"contributing/#automated-changelogs","text":"Release Drafter is used to automate release note updates as Pull Requests are opened to main . The configuration for Release Drafter exists in the .github/release-drafter.yml file and uses GitHub Actions.","title":"Automated Changelogs"},{"location":"contributing/create-new-library/","text":"Create A New Library \u00b6 This page will outline the steps for adding a new SDP library that's ready for others to consume and contribute to. Ask yourself some questions \u00b6 Does the library already exist? :) Could this functionality be added to an existing library? Is this library going to be usable by anyone else who wants to use this tool? If the library doesn't already exist, wouldn't make more sense as an addition to an existing library, and represents a use case that will be applicable outside of your current situation then it's likely a good candidate for contribution! Fork the repository \u00b6 This project follows a standard Fork Contribution Model , so if you haven't, go ahead and fork the SDP Pipeline Libraries Repository . Determine a name for the library \u00b6 A library's name is determined by the name of the directory that's going to contain the implemented steps The name should be all lowercase, snake_case, and the same as the tool or process being integrated # from the root of the repository just create <name_of_new_library> Implement the library's steps \u00b6 Go on over to JTE 's Library Development documentation to learn how to create libraries. There are a few conventions the SDP Pipeline Libraries have adopted, outlined below: Review the sdp library's helper methods \u00b6 The SDP library exists to implement common functionality required by other libraries. It's worthwhile to see if any of those steps are going to be useful to you during library development. Add a container image (if necessary) \u00b6 The SDP Pipeline Libraries try to install as few plugins on the Jenkins instance and as few tools on the underlying infrastructure as possible. Part of the pipeline runs inside container images, leveraging them as runtime pipeline environments. The existing container images used for this purpose can be found in the SDP Pipeline Images repository. If your library requires runtime dependencies, like a CLI , capture them in a container image and open a PR to the SDP Pipeline Images repository. In your step implementations, the image that's used should be overrideable but default to the image hosted via GitHub Package Registry on the SDP Pipeline Images repository. Note If your library requires runtime dependencies, your new library won't be accepted until the required image has been merged and published to the SDP Pipeline Images repository. Add documentation for the library \u00b6 Create the documentation page \u00b6 Libraries are required to have a documentation page to be accepted. To keep the library documentation consistent, copy the resources/README.template.md is copied into a new library's README.md as a starting point to fill in. Update the landing page libraries table \u00b6 The landing page for the SDP Pipeline Libraries contains a table that outlines each library and a high-level description of the library. To make sure new library descriptions are added to the list, be sure to fill out the Frontmatter description block at the top of the new library's README.md file: --- description: --- Preview your documentation \u00b6 You can run just serve at the root of the repository to build the documentation as static HTML, and view it at . Add unit tests \u00b6 It's highly encouraged that unit tests be written for the library. Tests should be placed in the test directory within the library directory Read the Unit Testing Documentation Write some tests for your steps Add a library configuration file \u00b6 To help prevent configuration errors, you can also validate the library parameters . Open a Pull Request \u00b6 The library is now ready for review! At this point you should have: A new library with steps implemented using the SDP library's helpers if necessary A new SDP Pipeline Image corresponding to the new library for its runtime dependencies (if necessary) Documentation for the library Unit tests for the library A strategy for validating the library's configuration parameters These will all be confirmed during PR review.","title":"Create A New Library"},{"location":"contributing/create-new-library/#create-a-new-library","text":"This page will outline the steps for adding a new SDP library that's ready for others to consume and contribute to.","title":"Create A New Library"},{"location":"contributing/create-new-library/#ask-yourself-some-questions","text":"Does the library already exist? :) Could this functionality be added to an existing library? Is this library going to be usable by anyone else who wants to use this tool? If the library doesn't already exist, wouldn't make more sense as an addition to an existing library, and represents a use case that will be applicable outside of your current situation then it's likely a good candidate for contribution!","title":"Ask yourself some questions"},{"location":"contributing/create-new-library/#fork-the-repository","text":"This project follows a standard Fork Contribution Model , so if you haven't, go ahead and fork the SDP Pipeline Libraries Repository .","title":"Fork the repository"},{"location":"contributing/create-new-library/#determine-a-name-for-the-library","text":"A library's name is determined by the name of the directory that's going to contain the implemented steps The name should be all lowercase, snake_case, and the same as the tool or process being integrated # from the root of the repository just create <name_of_new_library>","title":"Determine a name for the library"},{"location":"contributing/create-new-library/#implement-the-librarys-steps","text":"Go on over to JTE 's Library Development documentation to learn how to create libraries. There are a few conventions the SDP Pipeline Libraries have adopted, outlined below:","title":"Implement the library's steps"},{"location":"contributing/create-new-library/#review-the-sdp-librarys-helper-methods","text":"The SDP library exists to implement common functionality required by other libraries. It's worthwhile to see if any of those steps are going to be useful to you during library development.","title":"Review the sdp library's helper methods"},{"location":"contributing/create-new-library/#add-a-container-image-if-necessary","text":"The SDP Pipeline Libraries try to install as few plugins on the Jenkins instance and as few tools on the underlying infrastructure as possible. Part of the pipeline runs inside container images, leveraging them as runtime pipeline environments. The existing container images used for this purpose can be found in the SDP Pipeline Images repository. If your library requires runtime dependencies, like a CLI , capture them in a container image and open a PR to the SDP Pipeline Images repository. In your step implementations, the image that's used should be overrideable but default to the image hosted via GitHub Package Registry on the SDP Pipeline Images repository. Note If your library requires runtime dependencies, your new library won't be accepted until the required image has been merged and published to the SDP Pipeline Images repository.","title":"Add a container image (if necessary)"},{"location":"contributing/create-new-library/#add-documentation-for-the-library","text":"","title":"Add documentation for the library"},{"location":"contributing/create-new-library/#create-the-documentation-page","text":"Libraries are required to have a documentation page to be accepted. To keep the library documentation consistent, copy the resources/README.template.md is copied into a new library's README.md as a starting point to fill in.","title":"Create the documentation page"},{"location":"contributing/create-new-library/#update-the-landing-page-libraries-table","text":"The landing page for the SDP Pipeline Libraries contains a table that outlines each library and a high-level description of the library. To make sure new library descriptions are added to the list, be sure to fill out the Frontmatter description block at the top of the new library's README.md file: --- description: ---","title":"Update the landing page libraries table"},{"location":"contributing/create-new-library/#preview-your-documentation","text":"You can run just serve at the root of the repository to build the documentation as static HTML, and view it at .","title":"Preview your documentation"},{"location":"contributing/create-new-library/#add-unit-tests","text":"It's highly encouraged that unit tests be written for the library. Tests should be placed in the test directory within the library directory Read the Unit Testing Documentation Write some tests for your steps","title":"Add unit tests"},{"location":"contributing/create-new-library/#add-a-library-configuration-file","text":"To help prevent configuration errors, you can also validate the library parameters .","title":"Add a library configuration file"},{"location":"contributing/create-new-library/#open-a-pull-request","text":"The library is now ready for review! At this point you should have: A new library with steps implemented using the SDP library's helpers if necessary A new SDP Pipeline Image corresponding to the new library for its runtime dependencies (if necessary) Documentation for the library Unit tests for the library A strategy for validating the library's configuration parameters These will all be confirmed during PR review.","title":"Open a Pull Request"},{"location":"how-to/overview/","text":"Overview \u00b6 How-To Guides are goal oriented step-by-step instructions for specific problems. How-To Guide Description Pin a Library Source to a Specific Release Teaches you how to pin a Library Source, such as the SDP Libraries to a specific release.","title":"Overview"},{"location":"how-to/overview/#overview","text":"How-To Guides are goal oriented step-by-step instructions for specific problems. How-To Guide Description Pin a Library Source to a Specific Release Teaches you how to pin a Library Source, such as the SDP Libraries to a specific release.","title":"Overview"},{"location":"how-to/pin-a-library-source-to-a-specific-release/","text":"Pin a Library Source to a Specific Release \u00b6 Breaking changes will sometimes be merged into the default branch of the SDP Libraries repository. Because of this, you may want to pin the Library Source to a specific release so that your pipelines don't break when the library is updated. Steps \u00b6 Navigate to your Jenkins dashboard. Click on \"Manage Jenkins\" on the left side panel. Click on \"Configure System\" under the \"System Configuration\" section. Scroll down to the \"Jenkins Templating Engine\" section of this page. Find the Library Source you want to pin to a specific release. Update the \"Branch Specifier\" field using the following format: refs/tags/{git tag} (replacing {git tag} with the tag you want to pin the Library Source to). Click \"Save\" to save the changes.","title":"Pin a Library Source to a Specific Release"},{"location":"how-to/pin-a-library-source-to-a-specific-release/#pin-a-library-source-to-a-specific-release","text":"Breaking changes will sometimes be merged into the default branch of the SDP Libraries repository. Because of this, you may want to pin the Library Source to a specific release so that your pipelines don't break when the library is updated.","title":"Pin a Library Source to a Specific Release"},{"location":"how-to/pin-a-library-source-to-a-specific-release/#steps","text":"Navigate to your Jenkins dashboard. Click on \"Manage Jenkins\" on the left side panel. Click on \"Configure System\" under the \"System Configuration\" section. Scroll down to the \"Jenkins Templating Engine\" section of this page. Find the Library Source you want to pin to a specific release. Update the \"Branch Specifier\" field using the following format: refs/tags/{git tag} (replacing {git tag} with the tag you want to pin the Library Source to). Click \"Save\" to save the changes.","title":"Steps"},{"location":"libraries/","text":"Overview \u00b6 Available Libraries \u00b6 Library Description a11y Leverages The A11y Machine to perform accessibility compliance scanning Anchore Performs comprehensive container image vulnerability scan and compliance policy evaluation using your Anchore Enterprise or Anchore Engine installation Cypress This library allows you to run end-to-end tests with Cypress Docker Uses docker to build and publish container images, tagging them with the Git SHA Docker Compose Uses docker compose to deploy and tear down containers DotNet This library allows you to perform .NET build and test commands in the SDP dotnet-sdk agent container Git Allows you to map a branching strategy to specific pipeline actions when using Public GitHub, GitLab, or GitHub Enterprise Google Lighthouse Performs accessibility compliance, performance, search engine optimization, and best practice validations on a frontend application Grype Uses the Grype CLI to scan container images for vulnerabilities. Kubernetes Allows you to perform deployments using Helm to a kubernetes cluster (or clusters) Maven This library allows you to perform Maven commands in a defined build agent container npm Run NPM script commands in an NVM container with a specified Node version OpenShift Allows you to perform deployments using Helm to a Red Hat OpenShift Container Platform (or platforms) OWASP Dependency Check Leverages OWASP Dependency Check for scanning third party application dependencies OWASP ZAP Leverages OWASP ZAP to perform penetration testing Protractor Leverages Protractor, a front-end unit testing utility, to perform unit tests PyTest Leverages PyTest, a Python unit testing library, to perform unit tests SDP An internal helper library that the others utilize Slack Facilitates pipeline notifications to the configured Slack channel SonarQube Performs static code analysis with SonarQube Syft This library allows you to generate a Software Bill of Materials (SBOM) for each container built in your project Sysdig Secure Performs container image scanning with Sysdig Secure's inline scanner Terraform Deploys Infrastructure as Code using Terraform Twistlock Performs container image scanning with TwistLock webhint A customizable linting tool that helps you improve your site's accessibility, speed, cross-browser compatibility, and more by checking your code for best practices and common errors Yarn Run Yarn script commands in an NVM container with a specified Node version","title":"Overview"},{"location":"libraries/#overview","text":"","title":"Overview"},{"location":"libraries/#available-libraries","text":"Library Description a11y Leverages The A11y Machine to perform accessibility compliance scanning Anchore Performs comprehensive container image vulnerability scan and compliance policy evaluation using your Anchore Enterprise or Anchore Engine installation Cypress This library allows you to run end-to-end tests with Cypress Docker Uses docker to build and publish container images, tagging them with the Git SHA Docker Compose Uses docker compose to deploy and tear down containers DotNet This library allows you to perform .NET build and test commands in the SDP dotnet-sdk agent container Git Allows you to map a branching strategy to specific pipeline actions when using Public GitHub, GitLab, or GitHub Enterprise Google Lighthouse Performs accessibility compliance, performance, search engine optimization, and best practice validations on a frontend application Grype Uses the Grype CLI to scan container images for vulnerabilities. Kubernetes Allows you to perform deployments using Helm to a kubernetes cluster (or clusters) Maven This library allows you to perform Maven commands in a defined build agent container npm Run NPM script commands in an NVM container with a specified Node version OpenShift Allows you to perform deployments using Helm to a Red Hat OpenShift Container Platform (or platforms) OWASP Dependency Check Leverages OWASP Dependency Check for scanning third party application dependencies OWASP ZAP Leverages OWASP ZAP to perform penetration testing Protractor Leverages Protractor, a front-end unit testing utility, to perform unit tests PyTest Leverages PyTest, a Python unit testing library, to perform unit tests SDP An internal helper library that the others utilize Slack Facilitates pipeline notifications to the configured Slack channel SonarQube Performs static code analysis with SonarQube Syft This library allows you to generate a Software Bill of Materials (SBOM) for each container built in your project Sysdig Secure Performs container image scanning with Sysdig Secure's inline scanner Terraform Deploys Infrastructure as Code using Terraform Twistlock Performs container image scanning with TwistLock webhint A customizable linting tool that helps you improve your site's accessibility, speed, cross-browser compatibility, and more by checking your code for best practices and common errors Yarn Run Yarn script commands in an NVM container with a specified Node version","title":"Available Libraries"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/","text":"a11y \u00b6 The A11y Machine is an automated accessibility testing tool which crawls and tests pages of a web application to produce detailed reports. It validates pages against the following specifications/laws: W3C Web Content Accessibility Guidelines (WCAG) 2.0, including A, AA and AAA levels ( understanding levels of conformance ) U.S. Section 508 legislation W3C HTML5 Recommendation Deprecated This library is no longer maintained because the A11y Machine is no longer maintained. Consider using webhint instead. Steps \u00b6 Step Description accessibility_compliance_test() crawls the provided website and performs accessibility compliance scanning Configuration \u00b6 Field Description Default Value URL The address a11y will crawl and scan A target URL can be given. However env.FRONTEND_URL supersedes all configurations. If no env.FRONTEND_URL is found then the provided target URL is used. If no URL is provided an error is thrown. libraries { a11y { url = \"https://example.com\" } } Results \u00b6 The results of the scan are captured in an HTML report that gets archived by Jenkins. Report Index \u00b6 Report from a specific address \u00b6 Dependencies \u00b6","title":"a11y"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#a11y","text":"The A11y Machine is an automated accessibility testing tool which crawls and tests pages of a web application to produce detailed reports. It validates pages against the following specifications/laws: W3C Web Content Accessibility Guidelines (WCAG) 2.0, including A, AA and AAA levels ( understanding levels of conformance ) U.S. Section 508 legislation W3C HTML5 Recommendation Deprecated This library is no longer maintained because the A11y Machine is no longer maintained. Consider using webhint instead.","title":"a11y"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#steps","text":"Step Description accessibility_compliance_test() crawls the provided website and performs accessibility compliance scanning","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#configuration","text":"Field Description Default Value URL The address a11y will crawl and scan A target URL can be given. However env.FRONTEND_URL supersedes all configurations. If no env.FRONTEND_URL is found then the provided target URL is used. If no URL is provided an error is thrown. libraries { a11y { url = \"https://example.com\" } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#results","text":"The results of the scan are captured in an HTML report that gets archived by Jenkins.","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#report-index","text":"","title":"Report Index"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#report-from-a-specific-address","text":"","title":"Report from a specific address"},{"location":"libraries/SDP%20Pipeline%20Libraries/a11y/#dependencies","text":"","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/","text":"Anchore \u00b6 The Anchore library implements a comprehensive container image vulnerability and compliance scan, and generates JSON reports as well as tabular output that can be reviewed as part of your container image build step. The library communicates with your on-premises Anchore Enterprise or Anchore Engine deployment the Anchore API. For more information on deploying and using Anchore, see the Anchore Documentation . Steps \u00b6 Step Description scan_container_image() Scan the container image built and pushed to a registry, with the image tag identifiers to scan fetched by get_images_to_build() add_registry_creds() Add container registry credentials to Anchore if they don't already exist, so it can pull an image from a private registry. Can run this step before scan_container_image to ensure Anchore has access to an image in a private registry. Configuration \u00b6 Field Type Description Default Value cred String Name of the Jenkins Credential that holds the username/password for authentication against your locally deployed Anchore Engine None (required to be specified) anchore_engine_url String Full address of your Anchore Engine API endpoint. Example: http://anchore.yourdomain.com:8228/v1/ None (required to be specified) policy_id String ID of the policy to use when performing policy evaluation. If specified, the policy ID must be present in your Anchore Engine system. default (will use the currently default/active policy configured in your Anchore Engine) image_wait_timeout Integer Number of seconds to wait for an image to complete analysis. 300 archive_only Boolean If set to true , instruct library to skip displaying vulnerability / policy evaluation results to standard output. false bail_on_fail Boolean If set to true , cause the library to fail the build if the Anchore Policy Evaluation step results in a 'STOP' final action. Leave this set to default ( true ) if you would like your build to fail when your Anchore Policy Evaluation is successful, but the image doesn't conform to your specified policy requirements. true perform_vulnerability_scan Boolean If set to true , cause the library to perform an Anchore Software Vulnerability scan and generate a report. true perform_policy_evaluation Boolean If set to true , cause the library to perform an Anchore Policy Evaluation compliance scan and generate a report. true docker_registry_credential_id String Credential id of private docker registry true docker_registry_name String Address of private docker registry true k8s_credential String Credential id of kubeconfig credential true k8s_context String Cluster context to use in kubeconfig true libraries { anchore { cred = \"anchore_admin\" anchore_engine_url = \"http://anchore.yourdomain.com:8228/v1/\" //policy_id = \"anchore_security_only\" //image_wait_timeout = 600 //archive_only = false //bail_on_fail = false //perform_vulnerability_scan = true //perform_policy_evaluation = true //docker_registry_credential_id = docker_registry //docker_registry_name = \"\" //k8s_credential //k8s_context } } Results \u00b6 Results for this library are directly displayed in tabular form in the output of the scan_container_image() step, and also stored in parsable/raw form in your job's workspace as anchore_vulnerabilities.json and anchore_policy_evaluations.json for the vulnerability scan and policy evaluation result, respectively. Dependencies \u00b6 The Anchore library requires that an on-premises Anchore Enterprise or Anchore Engine deployment is up, configured, and running, as the library acts as a client against the Anchore API. Any image that's to be scanned must first be pushed to a registry that's also accessible to the Anchore Engine deployment (with registry credentials added if needed via regular Anchore Engine mechanisms for accessing registries). For more information on deploying Anchore Engine, see the Anchore Documentation . Troubleshooting \u00b6 The library will output both the raw HTTP as well as any JSON error payloads that may be returned when attempting to access the Anchore API. As this library is mostly a client, typically issues will be due to a configuration or other problem with the Anchore Engine installation. See the Anchore Troubleshooting Guide for help interpreting Anchore Engine error responses and common configuration issues.","title":"Anchore"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#anchore","text":"The Anchore library implements a comprehensive container image vulnerability and compliance scan, and generates JSON reports as well as tabular output that can be reviewed as part of your container image build step. The library communicates with your on-premises Anchore Enterprise or Anchore Engine deployment the Anchore API. For more information on deploying and using Anchore, see the Anchore Documentation .","title":"Anchore"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#steps","text":"Step Description scan_container_image() Scan the container image built and pushed to a registry, with the image tag identifiers to scan fetched by get_images_to_build() add_registry_creds() Add container registry credentials to Anchore if they don't already exist, so it can pull an image from a private registry. Can run this step before scan_container_image to ensure Anchore has access to an image in a private registry.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#configuration","text":"Field Type Description Default Value cred String Name of the Jenkins Credential that holds the username/password for authentication against your locally deployed Anchore Engine None (required to be specified) anchore_engine_url String Full address of your Anchore Engine API endpoint. Example: http://anchore.yourdomain.com:8228/v1/ None (required to be specified) policy_id String ID of the policy to use when performing policy evaluation. If specified, the policy ID must be present in your Anchore Engine system. default (will use the currently default/active policy configured in your Anchore Engine) image_wait_timeout Integer Number of seconds to wait for an image to complete analysis. 300 archive_only Boolean If set to true , instruct library to skip displaying vulnerability / policy evaluation results to standard output. false bail_on_fail Boolean If set to true , cause the library to fail the build if the Anchore Policy Evaluation step results in a 'STOP' final action. Leave this set to default ( true ) if you would like your build to fail when your Anchore Policy Evaluation is successful, but the image doesn't conform to your specified policy requirements. true perform_vulnerability_scan Boolean If set to true , cause the library to perform an Anchore Software Vulnerability scan and generate a report. true perform_policy_evaluation Boolean If set to true , cause the library to perform an Anchore Policy Evaluation compliance scan and generate a report. true docker_registry_credential_id String Credential id of private docker registry true docker_registry_name String Address of private docker registry true k8s_credential String Credential id of kubeconfig credential true k8s_context String Cluster context to use in kubeconfig true libraries { anchore { cred = \"anchore_admin\" anchore_engine_url = \"http://anchore.yourdomain.com:8228/v1/\" //policy_id = \"anchore_security_only\" //image_wait_timeout = 600 //archive_only = false //bail_on_fail = false //perform_vulnerability_scan = true //perform_policy_evaluation = true //docker_registry_credential_id = docker_registry //docker_registry_name = \"\" //k8s_credential //k8s_context } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#results","text":"Results for this library are directly displayed in tabular form in the output of the scan_container_image() step, and also stored in parsable/raw form in your job's workspace as anchore_vulnerabilities.json and anchore_policy_evaluations.json for the vulnerability scan and policy evaluation result, respectively.","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#dependencies","text":"The Anchore library requires that an on-premises Anchore Enterprise or Anchore Engine deployment is up, configured, and running, as the library acts as a client against the Anchore API. Any image that's to be scanned must first be pushed to a registry that's also accessible to the Anchore Engine deployment (with registry credentials added if needed via regular Anchore Engine mechanisms for accessing registries). For more information on deploying Anchore Engine, see the Anchore Documentation .","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/anchore/#troubleshooting","text":"The library will output both the raw HTTP as well as any JSON error payloads that may be returned when attempting to access the Anchore API. As this library is mostly a client, typically issues will be due to a configuration or other problem with the Anchore Engine installation. See the Anchore Troubleshooting Guide for help interpreting Anchore Engine error responses and common configuration issues.","title":"Troubleshooting"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/","text":"Cypress \u00b6 This library allows you to run end-to-end tests with Cypress . Steps \u00b6 Step Description end_to_end_test() Runs tests defined in the configured npm_script in your project package.json file Configuration \u00b6 Field Description Default Value npm_script Cypress NPM script to run (defined in your package.json file) N/A (Required) report_path Path where Cypress reports can be found after tests are run (will be archived in Jenkins and accepts Ant-style wildcards) N/A (Required) test_repo Repository containing Cypress test code (leave as default if test code is in the same repository as your project) . test_repo_creds Test code repository credentials '' branch If using a separate test_repo for Cypress test code, allows you to define the repository branch to use main container_image Cypress test runner container image to use cypress/browsers:node14.17.0-chrome91-ff89 container_registry Container registry to use (if not hub.docker.com) https://index.docker.io/v1/ container_registry_creds Container registry credentials to use '' Example Configuration Snippet \u00b6 pipeline_config.groovy libraries { cypress { npm_script = 'npm run cy:run:myTestSuite' report_path = 'cypress/reports/**' test_repo = 'https://github.com/username/my-cypress-test-repository' test_repo_creds = 'my-github-creds' branch = 'main' container_image = 'cypress/browsers:node14.17.0-chrome91-ff89' container_registry = 'https://index.docker.io/v1/' container_registry_creds = 'my-registry-creds' } } Dependencies \u00b6 None","title":"Cypress"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/#cypress","text":"This library allows you to run end-to-end tests with Cypress .","title":"Cypress"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/#steps","text":"Step Description end_to_end_test() Runs tests defined in the configured npm_script in your project package.json file","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/#configuration","text":"Field Description Default Value npm_script Cypress NPM script to run (defined in your package.json file) N/A (Required) report_path Path where Cypress reports can be found after tests are run (will be archived in Jenkins and accepts Ant-style wildcards) N/A (Required) test_repo Repository containing Cypress test code (leave as default if test code is in the same repository as your project) . test_repo_creds Test code repository credentials '' branch If using a separate test_repo for Cypress test code, allows you to define the repository branch to use main container_image Cypress test runner container image to use cypress/browsers:node14.17.0-chrome91-ff89 container_registry Container registry to use (if not hub.docker.com) https://index.docker.io/v1/ container_registry_creds Container registry credentials to use ''","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/#example-configuration-snippet","text":"pipeline_config.groovy libraries { cypress { npm_script = 'npm run cy:run:myTestSuite' report_path = 'cypress/reports/**' test_repo = 'https://github.com/username/my-cypress-test-repository' test_repo_creds = 'my-github-creds' branch = 'main' container_image = 'cypress/browsers:node14.17.0-chrome91-ff89' container_registry = 'https://index.docker.io/v1/' container_registry_creds = 'my-registry-creds' } }","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/cypress/#dependencies","text":"None","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/","text":"Docker \u00b6 The Docker library will build Docker images and push them into a Docker repository. Steps \u00b6 Step Description build() builds a container image, tagging it with the Git SHA, and pushes the image to the defined registry buildx() builds a multi-architecture image using Buildx emulation, and pushes the image or images to the defined registry get_images_to_build() inspects the source code repository based upon the configured build_strategy to determine which container images to build login_to_registry() logs in to the configured container registry retag() retags the container images determined by get_images_to_build() Example Configuration Snippets \u00b6 libraries { docker { build_strategy = \"dockerfile\" registry = \"docker-registry.default.svc:5000\" cred = \"openshift-docker-registry\" repo_path_prefix = \"proj-images\" image_name = \"my-container-image\" remove_local_image = true build_args { GITHUB_TOKEN { type = \"credential\" id = \"github_token\" } SOME_VALUE = \"some-inline-value-here\" } } } libraries { docker { build_strategy = 'dockerfiles' registry = \"docker-registry.default.svc:5000\" cred = \"openshift-docker-registry\" repo_path_prefix = \"proj-images\" image_name = \"my-container-image\" remove_local_image = true dockerfiles { backend { context = '.' dockerfile = 'Dockerfile.backend' } frontend { context = '.' dockerfile = 'Dockerfile.frontend' } } } } Configuration \u00b6 Field Description Default Value Required build_strategy Sets how the library will build the container image(s); must be dockerfile , dockerfiles , docker-compose , or modules dockerfile No registry Where the container images produced during the pipeline builds are going to be pushed to Yes registry_protocol the protocol to prepend to the registry when authenticating to the container registry \"https://\" No cred Credentials used for the repository where different docker pipeline tools are stored Yes repo_path_prefix The part of the repository name between the registry name and the last forward-slash \"\" No image_name Name of the container image being built env.REPO_NAME No remove_local_image Determines if the pipeline should remove the local image after building or retagging false No build_args A block of build arguments to pass to docker build (for more information, see below) No setExperimentalFlag If the docker version only has buildx as an experimental feature then this allows that flag to be set false No same_repo_different_tags When building multiple images don't change the repository name but append the key name to the tag false No dockerfiles A map of Dockerfiles to build for the dockerfiles build strategy No buildx[].name { } the key name to the map of the specific element of the buildx array Yes buildx[].useLatestTag Add an additional latest tag to the image being built on top of the other tag false No buildx[].tag Override the tag with a string Git SHA from commit No buildx[].context Dockerfile context for that image \".\" No buildx[].dockerfile_path Dockerfile location and name for that image \"Dockerfile\" No buildx[].platforms array of platforms to be built for that image linux/amd64 No buildx[].build_args A block of build arguments to pass for that element to docker buildx (for more information, see below) Build Arguments \u00b6 Static Inline Build Arguments \u00b6 To pass static values as build arguments, set a field within the configuration block where the key is the build argument name and the value is the build argument value. For example, libraries { docker { build_args { BUILD_ARG_NAME = \"some-inline-argument\" // (1) } } } This configuration would result in --build-arg BUILD_ARG_NAME='some-inline-argument' being passed to docker build Secret Text Credentials \u00b6 To pass a secret value, ensure that a Secret Text credential type has been created and fetch the credential id from the Jenkins credential store. libraries { docker { build_args { GITHUB_TOKEN { // (1) type = \"credential\" // (2) id = \"theCredentialId\" // (3) } } } } This will result in the build argument --build-arg GITHUB_TOKEN=<secret text> being passed to docker build . The library will mask the value of the secret from the build log. The type of credential must be set. This gives the library flexibility in the future to support other build argument types. This credential must exist and be a Secret Text credential in the Jenkins credential store. The library could be extended in the future to support other types of credentials, when necessary. Buildx Configuration \u00b6 Buildx Overview \u00b6 Go to Docker Buildx to learn more about Buildx and the requirements for it. The build strategy must be set to 'buildx' to use the buildx() step. Use Cases \u00b6 This step provides covers three use cases for building multi-architecture. 1. Single docker image name with one tag \u00b6 Example: repo/example:1.0 which supports amd64 , arm64 , armv7 Use this when the pipeline can build multiple architectures into a single docker image manifest. This method of building the image requires that the base image also supports all the architectures that the pipeline is building for. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" buildx { name { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] useLatestTag = true } } } } Generated buildx command from above: docker buildx build . -t docker-registry.default.svc:5000/java/example:<insert git sha> -t docker-registry.default.svc:5000/java/example:latest --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push 2. Single docker image name with multiple tags \u00b6 Example: repo/example:1.0-amd64 repo/example:1.0-arm64 where each image supports a different architecture Use this when there is no multi-architecture base image that can be used to build a single image manifest. Buildx is an array of maps that are separated by unique keys. This allows the pipeline to use the same Dockerfile with a parameterized base image or multiple Dockerfiles. This method requires that the same_repo_different_tags flag is set to true and for each element key in buildx to be unique. There can only be one element that can use the useLatestTag as it will throw an error due to the pipeline attempting to overwrite another image being built. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" same_repo_different_tags = true buildx { amd64 { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" ] useLatestTag = true tag = \"1.0\" } arm64 { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/arm64\" ] tag = \"1.0\" } } } } Generated buildx command from above: docker buildx build . -t docker-registry.default.svc:5000/java/example:1.0-amd64 -t docker-registry.default.svc:5000/java/example:latest --platform = linux/amd64 --build-arg = BASE_IMAGE = alpine:3.12 --push docker buildx build . -t docker-registry.default.svc:5000/java/example:1.0-arm64 --platform = linux/arm64 --build-arg = BASE_IMAGE = alpine:3.12 --push 3. Multiple docker image names with multiple tags \u00b6 Example: example-big:1.0 and example-small:1.0 where each image has its own list of architectures Use this when there is a single repository with multiple images that need to be built for multiple architectures. Each element's key must be unique for this to build or else it will override previous images and fail. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" buildx { jre { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] tag = \"1.0\" } jdk { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] tag = \"1.0\" } } } } Generated buildx commands from above: docker buildx build ./jdk -t docker-registry.default.svc:5000/java/example-jdk:1.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push docker buildx build ./jre -t docker-registry.default.svc:5000/java/example-jre:1.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push Dependencies \u00b6 A Docker registry must be set up and configured. Credentials to the repository are also needed. Either the github or github_enterprise library needs to be loaded as a library inside your pipeline_config.groovy file. Pipelines that use the buildx step need to be built on a node that has the correct Docker version with buildx support and the required emulator set up. See Docker Buildx for how to set up a node with the right configurations. Buildx enabled nodes need to be set up with BuildKit builders that support the architectures required for the step to work.","title":"Docker"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#docker","text":"The Docker library will build Docker images and push them into a Docker repository.","title":"Docker"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#steps","text":"Step Description build() builds a container image, tagging it with the Git SHA, and pushes the image to the defined registry buildx() builds a multi-architecture image using Buildx emulation, and pushes the image or images to the defined registry get_images_to_build() inspects the source code repository based upon the configured build_strategy to determine which container images to build login_to_registry() logs in to the configured container registry retag() retags the container images determined by get_images_to_build()","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#example-configuration-snippets","text":"libraries { docker { build_strategy = \"dockerfile\" registry = \"docker-registry.default.svc:5000\" cred = \"openshift-docker-registry\" repo_path_prefix = \"proj-images\" image_name = \"my-container-image\" remove_local_image = true build_args { GITHUB_TOKEN { type = \"credential\" id = \"github_token\" } SOME_VALUE = \"some-inline-value-here\" } } } libraries { docker { build_strategy = 'dockerfiles' registry = \"docker-registry.default.svc:5000\" cred = \"openshift-docker-registry\" repo_path_prefix = \"proj-images\" image_name = \"my-container-image\" remove_local_image = true dockerfiles { backend { context = '.' dockerfile = 'Dockerfile.backend' } frontend { context = '.' dockerfile = 'Dockerfile.frontend' } } } }","title":"Example Configuration Snippets"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#configuration","text":"Field Description Default Value Required build_strategy Sets how the library will build the container image(s); must be dockerfile , dockerfiles , docker-compose , or modules dockerfile No registry Where the container images produced during the pipeline builds are going to be pushed to Yes registry_protocol the protocol to prepend to the registry when authenticating to the container registry \"https://\" No cred Credentials used for the repository where different docker pipeline tools are stored Yes repo_path_prefix The part of the repository name between the registry name and the last forward-slash \"\" No image_name Name of the container image being built env.REPO_NAME No remove_local_image Determines if the pipeline should remove the local image after building or retagging false No build_args A block of build arguments to pass to docker build (for more information, see below) No setExperimentalFlag If the docker version only has buildx as an experimental feature then this allows that flag to be set false No same_repo_different_tags When building multiple images don't change the repository name but append the key name to the tag false No dockerfiles A map of Dockerfiles to build for the dockerfiles build strategy No buildx[].name { } the key name to the map of the specific element of the buildx array Yes buildx[].useLatestTag Add an additional latest tag to the image being built on top of the other tag false No buildx[].tag Override the tag with a string Git SHA from commit No buildx[].context Dockerfile context for that image \".\" No buildx[].dockerfile_path Dockerfile location and name for that image \"Dockerfile\" No buildx[].platforms array of platforms to be built for that image linux/amd64 No buildx[].build_args A block of build arguments to pass for that element to docker buildx (for more information, see below)","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#build-arguments","text":"","title":"Build Arguments"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#static-inline-build-arguments","text":"To pass static values as build arguments, set a field within the configuration block where the key is the build argument name and the value is the build argument value. For example, libraries { docker { build_args { BUILD_ARG_NAME = \"some-inline-argument\" // (1) } } } This configuration would result in --build-arg BUILD_ARG_NAME='some-inline-argument' being passed to docker build","title":"Static Inline Build Arguments"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#secret-text-credentials","text":"To pass a secret value, ensure that a Secret Text credential type has been created and fetch the credential id from the Jenkins credential store. libraries { docker { build_args { GITHUB_TOKEN { // (1) type = \"credential\" // (2) id = \"theCredentialId\" // (3) } } } } This will result in the build argument --build-arg GITHUB_TOKEN=<secret text> being passed to docker build . The library will mask the value of the secret from the build log. The type of credential must be set. This gives the library flexibility in the future to support other build argument types. This credential must exist and be a Secret Text credential in the Jenkins credential store. The library could be extended in the future to support other types of credentials, when necessary.","title":"Secret Text Credentials"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#buildx-configuration","text":"","title":"Buildx Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#buildx-overview","text":"Go to Docker Buildx to learn more about Buildx and the requirements for it. The build strategy must be set to 'buildx' to use the buildx() step.","title":"Buildx Overview"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#use-cases","text":"This step provides covers three use cases for building multi-architecture.","title":"Use Cases"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#1-single-docker-image-name-with-one-tag","text":"Example: repo/example:1.0 which supports amd64 , arm64 , armv7 Use this when the pipeline can build multiple architectures into a single docker image manifest. This method of building the image requires that the base image also supports all the architectures that the pipeline is building for. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" buildx { name { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] useLatestTag = true } } } } Generated buildx command from above: docker buildx build . -t docker-registry.default.svc:5000/java/example:<insert git sha> -t docker-registry.default.svc:5000/java/example:latest --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push","title":"1. Single docker image name with one tag"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#2-single-docker-image-name-with-multiple-tags","text":"Example: repo/example:1.0-amd64 repo/example:1.0-arm64 where each image supports a different architecture Use this when there is no multi-architecture base image that can be used to build a single image manifest. Buildx is an array of maps that are separated by unique keys. This allows the pipeline to use the same Dockerfile with a parameterized base image or multiple Dockerfiles. This method requires that the same_repo_different_tags flag is set to true and for each element key in buildx to be unique. There can only be one element that can use the useLatestTag as it will throw an error due to the pipeline attempting to overwrite another image being built. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" same_repo_different_tags = true buildx { amd64 { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" ] useLatestTag = true tag = \"1.0\" } arm64 { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/arm64\" ] tag = \"1.0\" } } } } Generated buildx command from above: docker buildx build . -t docker-registry.default.svc:5000/java/example:1.0-amd64 -t docker-registry.default.svc:5000/java/example:latest --platform = linux/amd64 --build-arg = BASE_IMAGE = alpine:3.12 --push docker buildx build . -t docker-registry.default.svc:5000/java/example:1.0-arm64 --platform = linux/arm64 --build-arg = BASE_IMAGE = alpine:3.12 --push","title":"2. Single docker image name with multiple tags"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#3-multiple-docker-image-names-with-multiple-tags","text":"Example: example-big:1.0 and example-small:1.0 where each image has its own list of architectures Use this when there is a single repository with multiple images that need to be built for multiple architectures. Each element's key must be unique for this to build or else it will override previous images and fail. Example Configuration Snippet for buildx Single docker image name with one tag: libraries { docker { build_strategy = \"buildx\" registry = \"docker-registry.default.svc:5000\" cred = \"docker_creds\" repo_path_prefix = \"java\" buildx { jre { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] tag = \"1.0\" } jdk { build_args { BASE_IMAGE = \"alpine:3.12\" } platforms = [ \"linux/amd64\" , \"linux/arm64\" , \"linux/arm/v7\" ] tag = \"1.0\" } } } } Generated buildx commands from above: docker buildx build ./jdk -t docker-registry.default.svc:5000/java/example-jdk:1.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push docker buildx build ./jre -t docker-registry.default.svc:5000/java/example-jre:1.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --build-arg = BASE_IMAGE = alpine:3.12 --push","title":"3. Multiple docker image names with multiple tags"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker/#dependencies","text":"A Docker registry must be set up and configured. Credentials to the repository are also needed. Either the github or github_enterprise library needs to be loaded as a library inside your pipeline_config.groovy file. Pipelines that use the buildx step need to be built on a node that has the correct Docker version with buildx support and the required emulator set up. See Docker Buildx for how to set up a node with the right configurations. Buildx enabled nodes need to be set up with BuildKit builders that support the architectures required for the step to work.","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/","text":"Docker Compose \u00b6 This library allows you to perform docker compose commands. Steps \u00b6 Step Description up() Runs docker-compose up with values taken from the configuration. down() Runs docker-compose down with values taken from the configuration. Example Usage \u00b6 compose . up () compose . down () Configuration \u00b6 The library configurations for docker_compose are as follows: Parameter Description files Optional list of ordered docker compose files to run. Omitting this parameter causes the command docker-compose up to run on a file named docker-compose.yml . env Optional environment file to pass to the docker-compose command. sleep Optional configuration that controls how long to wait after running the up() command before continuing the pipeline execution. This is helpful when the Docker containers need to be started before other steps, like integration tests, may run. Example Library Configuration \u00b6 libraries { docker_compose { files = [ \"docker-compose.it.yml\" ] env = \".env.ci\" sleep { time: 1 unit: \"MINUTES\" } } } Dependencies \u00b6 Docker and docker-compose installed on Jenkins.","title":"Docker Compose"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#docker-compose","text":"This library allows you to perform docker compose commands.","title":"Docker Compose"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#steps","text":"Step Description up() Runs docker-compose up with values taken from the configuration. down() Runs docker-compose down with values taken from the configuration.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#example-usage","text":"compose . up () compose . down ()","title":"Example Usage"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#configuration","text":"The library configurations for docker_compose are as follows: Parameter Description files Optional list of ordered docker compose files to run. Omitting this parameter causes the command docker-compose up to run on a file named docker-compose.yml . env Optional environment file to pass to the docker-compose command. sleep Optional configuration that controls how long to wait after running the up() command before continuing the pipeline execution. This is helpful when the Docker containers need to be started before other steps, like integration tests, may run.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#example-library-configuration","text":"libraries { docker_compose { files = [ \"docker-compose.it.yml\" ] env = \".env.ci\" sleep { time: 1 unit: \"MINUTES\" } } }","title":"Example Library Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/docker_compose/#dependencies","text":"Docker and docker-compose installed on Jenkins.","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/dotnet/","text":"DotNet \u00b6 This library allows you to perform .NET build and test commands in the SDP dotnet-sdk agent container. Steps \u00b6 Step Description source_build This step leverages the dotnet publish command to build your application and output the results to the specified directory via outDir variable. outDir defaults to a folder named \"bin.\" The specified folder is archived as a Jenkins artifact. unit_test This step leverages the dotnet test command to run the unit, integration and functional tests specified in the application repository and outputs the results to a specified directory via resultDir variable. resultDir defaults to a folder named \"coverage.\" The specified folder is archived as a Jenkins artifact. Configuration \u00b6 pipeline_config.groovy libraries { dotnet { sdk_image = 'dotnet-sdk:6.0.106' source_build { outDir = \"applicationOutput\" } unit_test { resultDir = \"Results\" } } } Dependencies \u00b6 The SDP library Access to a dotnet-sdk build agent container via the repository defined in your SDP library configuration","title":"DotNet"},{"location":"libraries/SDP%20Pipeline%20Libraries/dotnet/#dotnet","text":"This library allows you to perform .NET build and test commands in the SDP dotnet-sdk agent container.","title":"DotNet"},{"location":"libraries/SDP%20Pipeline%20Libraries/dotnet/#steps","text":"Step Description source_build This step leverages the dotnet publish command to build your application and output the results to the specified directory via outDir variable. outDir defaults to a folder named \"bin.\" The specified folder is archived as a Jenkins artifact. unit_test This step leverages the dotnet test command to run the unit, integration and functional tests specified in the application repository and outputs the results to a specified directory via resultDir variable. resultDir defaults to a folder named \"coverage.\" The specified folder is archived as a Jenkins artifact.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/dotnet/#configuration","text":"pipeline_config.groovy libraries { dotnet { sdk_image = 'dotnet-sdk:6.0.106' source_build { outDir = \"applicationOutput\" } unit_test { resultDir = \"Results\" } } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/dotnet/#dependencies","text":"The SDP library Access to a dotnet-sdk build agent container via the repository defined in your SDP library configuration","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/","text":"Git \u00b6 This library is unique in that rather than provide functional step implementations, it provides methods that help with the business logic defined within pipeline templates. Note It also provides additional functionality that can be useful for library developers to get SCM metadata or interact with a remote GitLab or GitHub repository. Configuration \u00b6 libraries { git { github github_enterprise gitlab { connection ( optional ) = String // (1) job_name ( optional ) = String job_status ( optional ) = [ \"pending\" , \"running\" , \"canceled\" , \"failed\" , \"success\" ] } } } Used for gitlab_status() Pipeline Template Business Logic \u00b6 The Git library contributes some helper methods to help with pipeline template orchestration. You can achieve fine grained control over what happens when in response to different Git events such as commits, merge requests, and merges. Git Flow Helper Methods Method Build Cause on_commit A direct commit to a branch on_merge_request A merge request was created or a developer pushed a commit to the source branch on_change A combination of on_commit and on_merge_request on_merge A merge request was merged into the branch These methods take named parameters to and from indicating direction of the git whose value is a regular expression to compare the branch names against. SDP recommends defining keywords for branch name regular expressions: keywords { master = /^[Mm]aster$/ develop = /^[Dd]evelop(ment\\|er\\|)$/ hotfix = /^[Hh]ot[Ff]ix-/ release = /^[Rr]elease-(d+.)*d$/ } Note These branch name regular expressions aren't a part of the Git library but rather leveraged by defining Keywords in the Pipeline Configuration File. SCM Specific Methods \u00b6 GitLab Methods Method Explanation gitlab_status Track Jenkins pipeline jobs in GitLab Example Pipeline Templates \u00b6 Full example using keywords \u00b6 on_commit { gitlab_status ( \"connection1\" , \"service-account\" , \"running\" ) continuous_integration () gitlab_status ( \"connection1\" , \"service-account\" , \"success\" ) } on_pull_request to: develop , { gitlab_status ( \"connection2\" , \"service-account\" , \"pending\" ) continuous_integration () gitlab_status ( \"connection2\" , \"service-account\" , \"running\" ) deploy_to dev parallel \"508 Testing\" : { accessibility_compliance_test () }, \"Functional Testing\" : { functional_test () }, \"Penetration Testing\" : { penetration_test () } deploy_to staging performance_test () gitlab_status ( \"connection2\" , \"service-account\" , \"success\" ) } on_merge to: master , from: develop , { gitlab_status ( \"connection\" , \"service-account2\" , \"running\" ) deploy_to prod smoke_test () gitlab_status ( \"connection\" , \"service-account2\" , \"success\" ) } Example using regular expressions directly \u00b6 on_commit to: /^[Ff]eature-.*/ , { // will be triggered on feature branches } on_merge_request from: /^[Ff]eature-.*/ , to: develop , { // will be triggered on PR's from feature to develop } Example using on_change \u00b6 on_change { // do CI on every commit or PR continuous_integration () } on_pull_request to: master , { // do some stuff on PR to master } on_merge to: master , { // PR was merged into master } External Dependencies \u00b6 gitlab-branch-source-plugin:1.4.4 if using GitLab","title":"Git"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#git","text":"This library is unique in that rather than provide functional step implementations, it provides methods that help with the business logic defined within pipeline templates. Note It also provides additional functionality that can be useful for library developers to get SCM metadata or interact with a remote GitLab or GitHub repository.","title":"Git"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#configuration","text":"libraries { git { github github_enterprise gitlab { connection ( optional ) = String // (1) job_name ( optional ) = String job_status ( optional ) = [ \"pending\" , \"running\" , \"canceled\" , \"failed\" , \"success\" ] } } } Used for gitlab_status()","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#pipeline-template-business-logic","text":"The Git library contributes some helper methods to help with pipeline template orchestration. You can achieve fine grained control over what happens when in response to different Git events such as commits, merge requests, and merges. Git Flow Helper Methods Method Build Cause on_commit A direct commit to a branch on_merge_request A merge request was created or a developer pushed a commit to the source branch on_change A combination of on_commit and on_merge_request on_merge A merge request was merged into the branch These methods take named parameters to and from indicating direction of the git whose value is a regular expression to compare the branch names against. SDP recommends defining keywords for branch name regular expressions: keywords { master = /^[Mm]aster$/ develop = /^[Dd]evelop(ment\\|er\\|)$/ hotfix = /^[Hh]ot[Ff]ix-/ release = /^[Rr]elease-(d+.)*d$/ } Note These branch name regular expressions aren't a part of the Git library but rather leveraged by defining Keywords in the Pipeline Configuration File.","title":"Pipeline Template Business Logic"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#scm-specific-methods","text":"GitLab Methods Method Explanation gitlab_status Track Jenkins pipeline jobs in GitLab","title":"SCM Specific Methods"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#example-pipeline-templates","text":"","title":"Example Pipeline Templates"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#full-example-using-keywords","text":"on_commit { gitlab_status ( \"connection1\" , \"service-account\" , \"running\" ) continuous_integration () gitlab_status ( \"connection1\" , \"service-account\" , \"success\" ) } on_pull_request to: develop , { gitlab_status ( \"connection2\" , \"service-account\" , \"pending\" ) continuous_integration () gitlab_status ( \"connection2\" , \"service-account\" , \"running\" ) deploy_to dev parallel \"508 Testing\" : { accessibility_compliance_test () }, \"Functional Testing\" : { functional_test () }, \"Penetration Testing\" : { penetration_test () } deploy_to staging performance_test () gitlab_status ( \"connection2\" , \"service-account\" , \"success\" ) } on_merge to: master , from: develop , { gitlab_status ( \"connection\" , \"service-account2\" , \"running\" ) deploy_to prod smoke_test () gitlab_status ( \"connection\" , \"service-account2\" , \"success\" ) }","title":"Full example using keywords"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#example-using-regular-expressions-directly","text":"on_commit to: /^[Ff]eature-.*/ , { // will be triggered on feature branches } on_merge_request from: /^[Ff]eature-.*/ , to: develop , { // will be triggered on PR's from feature to develop }","title":"Example using regular expressions directly"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#example-using-on_change","text":"on_change { // do CI on every commit or PR continuous_integration () } on_pull_request to: master , { // do some stuff on PR to master } on_merge to: master , { // PR was merged into master }","title":"Example using on_change"},{"location":"libraries/SDP%20Pipeline%20Libraries/git/#external-dependencies","text":"gitlab-branch-source-plugin:1.4.4 if using GitLab","title":"External Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/","text":"Google Lighthouse \u00b6 This library integrates Google Lighthouse to scan a frontend application for performance, accessibility compliance, search engine optimization, and best practice violations. The great part about this library is that developers can also use Google Lighthouse when developing locally in Chrome and these practices can be enforced via the pipeline. Steps \u00b6 Step Description accessibility_compliance_scan() performs a lighthouse analysis against the configured address Configuration \u00b6 Field Type Description Default Value url String The address to scan thresholds.performance.fail Double Performance scores less than or equal to this will fail the build 49.0 thresholds.performance.warn Double Performance above the failure threshold but less than this will mark the build unstable 89.0 thresholds.accessibility.fail Double Accessibility scores less than or equal to this will fail the build 49.0 thresholds.accessibility.warn Double Accessibility above the failure threshold but less than this will mark the build unstable 89.0 thresholds.best_practices.fail Double Best Practice scores less than or equal to this will fail the build 49.0 thresholds.best_practices.warn Double Best Practice above the failure threshold but less than this will mark the build unstable 89.0 thresholds.search_engine_optimization.fail Double Search Engine Optimization scores less than or equal to this will fail the build 49.0 thresholds.search_engine_optimization.warn Double Search Engine Optimization above the failure threshold but less than this will mark the build unstable 89.0 libraries { google_lighthouse { url = \"https://google.com\" thresholds { performance { fail = 75 } } } } Results \u00b6 An example HTML report generated by this library is available as a PDF here . Dependencies \u00b6 network connectivity from the Jenkins build agent running the scan to the provided address","title":"Google Lighthouse"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/#google-lighthouse","text":"This library integrates Google Lighthouse to scan a frontend application for performance, accessibility compliance, search engine optimization, and best practice violations. The great part about this library is that developers can also use Google Lighthouse when developing locally in Chrome and these practices can be enforced via the pipeline.","title":"Google Lighthouse"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/#steps","text":"Step Description accessibility_compliance_scan() performs a lighthouse analysis against the configured address","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/#configuration","text":"Field Type Description Default Value url String The address to scan thresholds.performance.fail Double Performance scores less than or equal to this will fail the build 49.0 thresholds.performance.warn Double Performance above the failure threshold but less than this will mark the build unstable 89.0 thresholds.accessibility.fail Double Accessibility scores less than or equal to this will fail the build 49.0 thresholds.accessibility.warn Double Accessibility above the failure threshold but less than this will mark the build unstable 89.0 thresholds.best_practices.fail Double Best Practice scores less than or equal to this will fail the build 49.0 thresholds.best_practices.warn Double Best Practice above the failure threshold but less than this will mark the build unstable 89.0 thresholds.search_engine_optimization.fail Double Search Engine Optimization scores less than or equal to this will fail the build 49.0 thresholds.search_engine_optimization.warn Double Search Engine Optimization above the failure threshold but less than this will mark the build unstable 89.0 libraries { google_lighthouse { url = \"https://google.com\" thresholds { performance { fail = 75 } } } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/#results","text":"An example HTML report generated by this library is available as a PDF here .","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/google_lighthouse/#dependencies","text":"network connectivity from the Jenkins build agent running the scan to the provided address","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/","text":"Grype \u00b6 Uses the Grype CLI to scan container images for vulnerabilities. Steps \u00b6 Step Description container_image_scan() Performs the Grype scan against your scaffold build image. Configuration \u00b6 Library Configuration Description Type Default Value Options grype_container The container image to execute the scan within String grype:0.38.0 report_format The output format of the generated report String json json , table , cyclonedx , template fail_on_severity The severity level threshold that will fail the pipeline String high none , negligible , low , medium , high , critical grype_config A custom path to a grype configuration file String null scan_sbom Boolean to turn on SBOM scanning Boolean false true, false pipeline_config.groovy libraries { grype { grype_container = \"grype:0.38.0\" report_format = \"json\" fail_on_severity = \"high\" grype_config = \"Path/to/Grype.yaml\" scan_sbom = false } } Grype Configuration File \u00b6 If grype_config isn't provided, the default locations for an application are .grype.yaml , .grype/config.yaml . Read the grype docs to learn more about the Grype configuration file Dependencies \u00b6 This library requires that the docker library also be loaded and build() be invoked before container_image_scan() If the default grype_container is replaced, it must be able to run docker containers (packages: docker-ce, docker-ce-cli and containerd.io).","title":"Grype"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/#grype","text":"Uses the Grype CLI to scan container images for vulnerabilities.","title":"Grype"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/#steps","text":"Step Description container_image_scan() Performs the Grype scan against your scaffold build image.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/#configuration","text":"Library Configuration Description Type Default Value Options grype_container The container image to execute the scan within String grype:0.38.0 report_format The output format of the generated report String json json , table , cyclonedx , template fail_on_severity The severity level threshold that will fail the pipeline String high none , negligible , low , medium , high , critical grype_config A custom path to a grype configuration file String null scan_sbom Boolean to turn on SBOM scanning Boolean false true, false pipeline_config.groovy libraries { grype { grype_container = \"grype:0.38.0\" report_format = \"json\" fail_on_severity = \"high\" grype_config = \"Path/to/Grype.yaml\" scan_sbom = false } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/#grype-configuration-file","text":"If grype_config isn't provided, the default locations for an application are .grype.yaml , .grype/config.yaml . Read the grype docs to learn more about the Grype configuration file","title":"Grype Configuration File"},{"location":"libraries/SDP%20Pipeline%20Libraries/grype/#dependencies","text":"This library requires that the docker library also be loaded and build() be invoked before container_image_scan() If the default grype_container is replaced, it must be able to run docker containers (packages: docker-ce, docker-ce-cli and containerd.io).","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/","text":"Kubernetes \u00b6 This library allows you to perform deployments to static or ephemeral Kubernetes application environments with Helm . Steps \u00b6 Step Description deploy_to() Performs a deployment using Helm ephemeral(Closure body, ApplicationEnvironment) Creates a short-lived application environment for testing Configuration \u00b6 The configurations for the Kubernetes library can be specified in the library spec or on a per application environment. Kubernetes Credential and Context \u00b6 The Kubernetes Credential is the Jenkins credential defined as a Secrets file that holds the kubeconfig file contents with access information to the Kubernetes target environments. The Kubernetes Context is the context within the kubeconfig that should be used to identify the target environment for deployment. You would specify this as follows: application_environments { dev { short_name = \"dev\" long_name = \"Development\" } test { short_name = \"test\" long_name = \"Test\" k8s_context = \"test\" } prod { short_name = \"prod\" long_name = \"Production\" k8s_credential = \"cluster1-config\" k8s_context = \"production\" } } libraries { kubernetes { k8s_credential = \"cluster2-config\" k8s_context = \"dev\" } } With this configuration, dev context within the cluster2-config would be used when deploying to dev test context within the cluster2-config would be used when deploying to test production context within the cluster1-config would be used when deploying to prod Together k8s_credential and k8s_context uniquely identify the target environment for deployment. Helm Configuration \u00b6 Helm is used for deployment into Kubernetes. Helm is a package manager and templating engine for Kubernetes manifests. Using Helm, the typical YAML manifests used to deploy to Kubernetes distributions can be templated for reuse. In this case, a different values file is used for each static application environment. Create Helm Configuration Repository \u00b6 You'll need to create a GitHub repository to store the Helm chart for your application(s). See the Helm docs on provisioning a new chart for how to initialize the repository with the skeleton for your chart. How you choose to build your Helm chart is up to you, you can put every API object in the templates directory or have subcharts for each individual microservice. Since SDP clones the GitHub repository and deploys the chart using the specified values file, it doesn't require any specific Helm chart repository structure. Values File Conventions* \u00b6 Given that container images are tagged using the Git SHA, SDP will clone your Helm configuration repository and update a key corresponding to the current version of each container image for each application environment. As such, a certain syntax is required in your values file. You must have an image_shas key. SDP will automatically add subkeys for each repositories under this image_shas with a value that's the Git SHA. Note Given that YAML keys can't have hyphens, hyphens in repository names will be replaced with underscores. image_shas: my_sample_application: abcdefgh another_repo: abcdef You can add any other keys necessary to appropriately parameterize your Helm chart. Helm Configurations for the Library \u00b6 The Helm configuration repository and GitHub credential can be configured globally in the library spec and overridden for specific application environments. The values file to will default to values.${app_env.short_name}.yaml , or can be overridden via app_env.chart_values_file . The name of the release will default to app_env.short_name , or can be overridden via app_env.release_name . An example of Helm configurations: application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { kubernetes { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" } } Promoting Images \u00b6 It's often beneficial to build a container image once, and then promote that image through different application environments. This makes it possible to test the content of an image once in a lower environment, and remain confident that the results of those tests would be the same as an image is promoted. Promoting images also speeds up the CI/CD pipeline, as building a container image is often the most time-consuming part of the pipeline. By default, the deploy_to() step of the kubernetes pipeline library will promote a container image if it can expect one to exist, which is when the most recent code change was a merge into the given code branch. The image would be expected to be built from an earlier commit, or while there was an open PR. You can override this default for the entire pipeline by setting the promote_previous_image config setting to false . You can also choose whether to promote images for each application environment individually through the promote_previous_image application_environment setting. This application_environment setting takes priority over the config setting. An example of these settings' usage: application_environments { dev { short_name = \"dev\" long_name = \"Development\" promote_previous_image = false } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { kubernetes { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" k8s_credential = \"cluster1-config\" k8s_context = \"staging\" promote_previous_image = true // (1) } } note: making this setting true is redundant, since true is the default Putting It All Together \u00b6 Kubernetes Library Configuration Options Field Description Default Value Defined On (Library Config or Application Environment) k8s_credential The Jenkins credential ID defined as a Secrets File that holds the kubeconfig file both helm_configuration_repository The GitHub Repository containing the Helm chart(s) for this application both helm_configuration_repository_branch The repository branch to fetch the Helm chart(s) from \"main\" both helm_configuration_repository_start_path The directory within the repository containing the Helm chart \".\" , which is the root of the repository both helm_configuration_repository_credential The Jenkins credential ID to access the Helm configuration GitHub repository both k8s_context The Jenkins credential ID specifying the context within the k8s_credential kubeconfig that identifies the target environment both chart_values_file The values file to use for the release Application Environment promote_previous_image Whether to promote a previously built image (Boolean) true both application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" k8s_credential = \"test-context\" } prod { short_name = \"prod\" long_name = \"Production\" k8s_credential = \"prod-clusters\" k8s_context = \"canary-context\" promote_previous_image = true } } libraries { kubernetes { k8s_credential = \"dev-test-clusters\" helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" k8s_credential = \"dev-context\" promote_previous_image = false } } Library Dependencies \u00b6 A library that implements the withGit method such as GitHub. External Dependencies \u00b6 Target Kubernetes cluster is deployed and accessible from Jenkins Helm configuration repository creates Values files contain the image_shas key convention A Jenkins credential exists to access Helm configuration repository A Jenkins credential exists holding the kubeconfig file A Jenkins credential exists specifying the current context within the kubeconfig","title":"Kubernetes"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#kubernetes","text":"This library allows you to perform deployments to static or ephemeral Kubernetes application environments with Helm .","title":"Kubernetes"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#steps","text":"Step Description deploy_to() Performs a deployment using Helm ephemeral(Closure body, ApplicationEnvironment) Creates a short-lived application environment for testing","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#configuration","text":"The configurations for the Kubernetes library can be specified in the library spec or on a per application environment.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#kubernetes-credential-and-context","text":"The Kubernetes Credential is the Jenkins credential defined as a Secrets file that holds the kubeconfig file contents with access information to the Kubernetes target environments. The Kubernetes Context is the context within the kubeconfig that should be used to identify the target environment for deployment. You would specify this as follows: application_environments { dev { short_name = \"dev\" long_name = \"Development\" } test { short_name = \"test\" long_name = \"Test\" k8s_context = \"test\" } prod { short_name = \"prod\" long_name = \"Production\" k8s_credential = \"cluster1-config\" k8s_context = \"production\" } } libraries { kubernetes { k8s_credential = \"cluster2-config\" k8s_context = \"dev\" } } With this configuration, dev context within the cluster2-config would be used when deploying to dev test context within the cluster2-config would be used when deploying to test production context within the cluster1-config would be used when deploying to prod Together k8s_credential and k8s_context uniquely identify the target environment for deployment.","title":"Kubernetes Credential and Context"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#helm-configuration","text":"Helm is used for deployment into Kubernetes. Helm is a package manager and templating engine for Kubernetes manifests. Using Helm, the typical YAML manifests used to deploy to Kubernetes distributions can be templated for reuse. In this case, a different values file is used for each static application environment.","title":"Helm Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#create-helm-configuration-repository","text":"You'll need to create a GitHub repository to store the Helm chart for your application(s). See the Helm docs on provisioning a new chart for how to initialize the repository with the skeleton for your chart. How you choose to build your Helm chart is up to you, you can put every API object in the templates directory or have subcharts for each individual microservice. Since SDP clones the GitHub repository and deploys the chart using the specified values file, it doesn't require any specific Helm chart repository structure.","title":"Create Helm Configuration Repository"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#values-file-conventions","text":"Given that container images are tagged using the Git SHA, SDP will clone your Helm configuration repository and update a key corresponding to the current version of each container image for each application environment. As such, a certain syntax is required in your values file. You must have an image_shas key. SDP will automatically add subkeys for each repositories under this image_shas with a value that's the Git SHA. Note Given that YAML keys can't have hyphens, hyphens in repository names will be replaced with underscores. image_shas: my_sample_application: abcdefgh another_repo: abcdef You can add any other keys necessary to appropriately parameterize your Helm chart.","title":"Values File Conventions*"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#helm-configurations-for-the-library","text":"The Helm configuration repository and GitHub credential can be configured globally in the library spec and overridden for specific application environments. The values file to will default to values.${app_env.short_name}.yaml , or can be overridden via app_env.chart_values_file . The name of the release will default to app_env.short_name , or can be overridden via app_env.release_name . An example of Helm configurations: application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { kubernetes { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" } }","title":"Helm Configurations for the Library"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#promoting-images","text":"It's often beneficial to build a container image once, and then promote that image through different application environments. This makes it possible to test the content of an image once in a lower environment, and remain confident that the results of those tests would be the same as an image is promoted. Promoting images also speeds up the CI/CD pipeline, as building a container image is often the most time-consuming part of the pipeline. By default, the deploy_to() step of the kubernetes pipeline library will promote a container image if it can expect one to exist, which is when the most recent code change was a merge into the given code branch. The image would be expected to be built from an earlier commit, or while there was an open PR. You can override this default for the entire pipeline by setting the promote_previous_image config setting to false . You can also choose whether to promote images for each application environment individually through the promote_previous_image application_environment setting. This application_environment setting takes priority over the config setting. An example of these settings' usage: application_environments { dev { short_name = \"dev\" long_name = \"Development\" promote_previous_image = false } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { kubernetes { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" k8s_credential = \"cluster1-config\" k8s_context = \"staging\" promote_previous_image = true // (1) } } note: making this setting true is redundant, since true is the default","title":"Promoting Images"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#putting-it-all-together","text":"Kubernetes Library Configuration Options Field Description Default Value Defined On (Library Config or Application Environment) k8s_credential The Jenkins credential ID defined as a Secrets File that holds the kubeconfig file both helm_configuration_repository The GitHub Repository containing the Helm chart(s) for this application both helm_configuration_repository_branch The repository branch to fetch the Helm chart(s) from \"main\" both helm_configuration_repository_start_path The directory within the repository containing the Helm chart \".\" , which is the root of the repository both helm_configuration_repository_credential The Jenkins credential ID to access the Helm configuration GitHub repository both k8s_context The Jenkins credential ID specifying the context within the k8s_credential kubeconfig that identifies the target environment both chart_values_file The values file to use for the release Application Environment promote_previous_image Whether to promote a previously built image (Boolean) true both application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" k8s_credential = \"test-context\" } prod { short_name = \"prod\" long_name = \"Production\" k8s_credential = \"prod-clusters\" k8s_context = \"canary-context\" promote_previous_image = true } } libraries { kubernetes { k8s_credential = \"dev-test-clusters\" helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" k8s_credential = \"dev-context\" promote_previous_image = false } }","title":"Putting It All Together"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#library-dependencies","text":"A library that implements the withGit method such as GitHub.","title":"Library Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/kubernetes/#external-dependencies","text":"Target Kubernetes cluster is deployed and accessible from Jenkins Helm configuration repository creates Values files contain the image_shas key convention A Jenkins credential exists to access Helm configuration repository A Jenkins credential exists holding the kubeconfig file A Jenkins credential exists specifying the current context within the kubeconfig","title":"External Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/","text":"Maven \u00b6 This library allows you to perform Maven commands in a defined build agent container. Steps \u00b6 Step Description [dynamic]() Step name can be any non-reserved word. This library uses dynamic step aliasing to run the Maven phases, goals, and options defined in the step configuration. Configuration \u00b6 pipeline_config.groovy libraries { maven { myMavenStep { stageName = 'Initial Maven Lifecycle' buildContainer = 'mvn:3.8.5-openjdk-11' phases = [ 'clean' , 'validate' ] goals = [ 'compiler:testCompile' ] options = [ '-q' ] secrets { myToken { type = 'text' name = 'token-name' id = 'my-token-id' } myCredentials { type = 'usernamePassword' usernameVar = 'USER' passwordVar = 'PASS' id = 'my-credentials-id' } } } anotherMavenStep { stageName = 'Maven Build' buildContainer = 'mvn' phases = [ 'build' ] artifacts = [ 'target/*.jar' ] } } } Dependencies \u00b6 The sdp library Access to an appropriate Maven build agent container via the repository defined in your sdp library configuration Migrating to 4.0 \u00b6 SDP 4.0 reworked this library to use dynamic step aliasing. The Maven tool configuration within Jenkins is no longer required to use this library. To recreate the previous maven.run() functionality of prior versions, the below minimal pipeline configuration and template can be used: Sample Pipeline Configuration \u00b6 Post-4.0 Pre-4.0 pipeline_config.groovy libraries { maven { build { stageName = \"Maven Build\" buildContainer = 'mvn' phases = [ 'clean' , 'install' ] options = [ '-P integration-test' ] } } } pipeline_config.groovy libraries { maven { mavenId = \"maven\" } } Sample Pipeline Template \u00b6 Post-4.0 Pre-4.0 Jenkinsfile build () Jenkinsfile maven . run ([ \"clean\" , \"install\" ], profiles: [ \"integration-test\" ])","title":"Maven"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#maven","text":"This library allows you to perform Maven commands in a defined build agent container.","title":"Maven"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#steps","text":"Step Description [dynamic]() Step name can be any non-reserved word. This library uses dynamic step aliasing to run the Maven phases, goals, and options defined in the step configuration.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#configuration","text":"pipeline_config.groovy libraries { maven { myMavenStep { stageName = 'Initial Maven Lifecycle' buildContainer = 'mvn:3.8.5-openjdk-11' phases = [ 'clean' , 'validate' ] goals = [ 'compiler:testCompile' ] options = [ '-q' ] secrets { myToken { type = 'text' name = 'token-name' id = 'my-token-id' } myCredentials { type = 'usernamePassword' usernameVar = 'USER' passwordVar = 'PASS' id = 'my-credentials-id' } } } anotherMavenStep { stageName = 'Maven Build' buildContainer = 'mvn' phases = [ 'build' ] artifacts = [ 'target/*.jar' ] } } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#dependencies","text":"The sdp library Access to an appropriate Maven build agent container via the repository defined in your sdp library configuration","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#migrating-to-40","text":"SDP 4.0 reworked this library to use dynamic step aliasing. The Maven tool configuration within Jenkins is no longer required to use this library. To recreate the previous maven.run() functionality of prior versions, the below minimal pipeline configuration and template can be used:","title":"Migrating to 4.0"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#sample-pipeline-configuration","text":"Post-4.0 Pre-4.0 pipeline_config.groovy libraries { maven { build { stageName = \"Maven Build\" buildContainer = 'mvn' phases = [ 'clean' , 'install' ] options = [ '-P integration-test' ] } } } pipeline_config.groovy libraries { maven { mavenId = \"maven\" } }","title":"Sample Pipeline Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/maven/#sample-pipeline-template","text":"Post-4.0 Pre-4.0 Jenkinsfile build () Jenkinsfile maven . run ([ \"clean\" , \"install\" ], profiles: [ \"integration-test\" ])","title":"Sample Pipeline Template"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/","text":"npm \u00b6 Run NPM script commands in an NVM container with a specified Node version. Configuration \u00b6 All configs can be set in either the library config or the Application Environment. All configs set in Application Environment take precedence. Environment variables and secrets set in the library config are concatenated with those set in the Application Environment. Environment variables and secrets with the same key are set to the definition contained in the Application Environment. Steps \u00b6 Steps are configured dynamically in either the library config or the Application Environment. pipeline_configuration.groovy libraries { npm { [ step_name ] { // config fields described below } ... } } Example Library Configuration \u00b6 Field Description Default nvm_container The container image to use nvm:1.0.0 node_version Node version to run NPM within (installed via NVM) lts/* <step name>.stageName stage name displayed in the Jenkins dashboard N/A <step name>.script NPM script ran by the step N/A <step name>.artifacts array of glob patterns for artifacts that should be archived <step name>.npmInstall NPM install command to run; npm install can be skipped with value \"skip\" ci <step name>.env environment variables to make available to the NPM process; can include key/value pairs and secrets [] <step name>.env.secrets text or username/password credentials to make available to the NPM process; must be present and available in Jenkins credential store [] <step name>.useEslintPlugin if the Jenkins ESLint Plugin is installed, will run the recordIssues step to send lint results to the plugin dashboard false Full Configuration Example \u00b6 Each available method has config options that can be specified in the Application Environment or within the library configuration. pipeline_configuration.groovy application_environments { dev prod { npm { node_version = \"14.16.1\" unit_test { stageName = \"NPM Unit Tests\" script = \"full-test-suite\" artifacts = [ \"coverage/lcov.info\" ] npmInstall = \"ci\" env { someKey = \"prodValue for tests\" // (1) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } // (2) } } } source_build { stageName = \"NPM Source Build\" script = \"prod-build\" env { someKey = \"prodValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } lint_code { stageName = \"NPM Lint Code\" script = \"lint\" artifacts = [ \"eslint-report.json\" , \"eslint-report.html\" , \"eslint-report.xml\" , ] useEslintPlugin = true env { someKey = \"prodValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } } } libraries { npm { node_version = \"lts/*\" unit_test { stageName = \"NPM Unit Tests\" script = \"test\" npmInstall = \"install\" env { someKey = \"someValue for tests\" // (3) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } // (4) } } } source_build { stageName = \"NPM Source Build\" script = \"build\" npmInstall = \"skip\" env { someKey = \"someValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } lint_code { stageName = \"NPM Lint Code\" script = \"lint\" npmInstall = \"skip\" env { someKey = \"someValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } } more envVars as needed more secrets as needed more envVars as needed more secrets as needed This example shows the prod Application Environment overriding configs set in the library config. source_build.npmInstall is preserved as set in library config, since it isn't overridden by the Application Environment. Minimal Configuration Example \u00b6 The minimal configuration for this library is: pipeline_configuration.groovy libraries { npm { unit_test { stageName = \"NPM Unit Tests\" script = \"test\" } } } Secrets \u00b6 There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored in the Jenkins credential store and be available to the pipeline. The name of each credential block (such as someTextCredential ) is arbitrary. It's just a key, used to supersede library config with Application Environment configs, and when describing configuration errors found by the step. Dependencies \u00b6 The SDP library must be loaded inside the pipeline_config.groovy file. Migrating from SDP 3.2 to 4.0 \u00b6 SDP 4.0 reworked this library to use dynamic step aliasing. To recreate the previous source_build() and unit_test() functionality of version 3.2 , the below minimal pipeline configuration can be used: pipeline_configuration.groovy libraries { npm { source_build { stageName = \"NPM Source Build\" script = \"build\" } unit_test { stageName = \"NPM Unit Tests\" script = \"test\" } } }","title":"npm"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#npm","text":"Run NPM script commands in an NVM container with a specified Node version.","title":"npm"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#configuration","text":"All configs can be set in either the library config or the Application Environment. All configs set in Application Environment take precedence. Environment variables and secrets set in the library config are concatenated with those set in the Application Environment. Environment variables and secrets with the same key are set to the definition contained in the Application Environment.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#steps","text":"Steps are configured dynamically in either the library config or the Application Environment. pipeline_configuration.groovy libraries { npm { [ step_name ] { // config fields described below } ... } }","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#example-library-configuration","text":"Field Description Default nvm_container The container image to use nvm:1.0.0 node_version Node version to run NPM within (installed via NVM) lts/* <step name>.stageName stage name displayed in the Jenkins dashboard N/A <step name>.script NPM script ran by the step N/A <step name>.artifacts array of glob patterns for artifacts that should be archived <step name>.npmInstall NPM install command to run; npm install can be skipped with value \"skip\" ci <step name>.env environment variables to make available to the NPM process; can include key/value pairs and secrets [] <step name>.env.secrets text or username/password credentials to make available to the NPM process; must be present and available in Jenkins credential store [] <step name>.useEslintPlugin if the Jenkins ESLint Plugin is installed, will run the recordIssues step to send lint results to the plugin dashboard false","title":"Example Library Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#full-configuration-example","text":"Each available method has config options that can be specified in the Application Environment or within the library configuration. pipeline_configuration.groovy application_environments { dev prod { npm { node_version = \"14.16.1\" unit_test { stageName = \"NPM Unit Tests\" script = \"full-test-suite\" artifacts = [ \"coverage/lcov.info\" ] npmInstall = \"ci\" env { someKey = \"prodValue for tests\" // (1) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } // (2) } } } source_build { stageName = \"NPM Source Build\" script = \"prod-build\" env { someKey = \"prodValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } lint_code { stageName = \"NPM Lint Code\" script = \"lint\" artifacts = [ \"eslint-report.json\" , \"eslint-report.html\" , \"eslint-report.xml\" , ] useEslintPlugin = true env { someKey = \"prodValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } } } libraries { npm { node_version = \"lts/*\" unit_test { stageName = \"NPM Unit Tests\" script = \"test\" npmInstall = \"install\" env { someKey = \"someValue for tests\" // (3) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } // (4) } } } source_build { stageName = \"NPM Source Build\" script = \"build\" npmInstall = \"skip\" env { someKey = \"someValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } lint_code { stageName = \"NPM Lint Code\" script = \"lint\" npmInstall = \"skip\" env { someKey = \"someValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } } more envVars as needed more secrets as needed more envVars as needed more secrets as needed This example shows the prod Application Environment overriding configs set in the library config. source_build.npmInstall is preserved as set in library config, since it isn't overridden by the Application Environment.","title":"Full Configuration Example"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#minimal-configuration-example","text":"The minimal configuration for this library is: pipeline_configuration.groovy libraries { npm { unit_test { stageName = \"NPM Unit Tests\" script = \"test\" } } }","title":"Minimal Configuration Example"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#secrets","text":"There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored in the Jenkins credential store and be available to the pipeline. The name of each credential block (such as someTextCredential ) is arbitrary. It's just a key, used to supersede library config with Application Environment configs, and when describing configuration errors found by the step.","title":"Secrets"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#dependencies","text":"The SDP library must be loaded inside the pipeline_config.groovy file.","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/npm/#migrating-from-sdp-32-to-40","text":"SDP 4.0 reworked this library to use dynamic step aliasing. To recreate the previous source_build() and unit_test() functionality of version 3.2 , the below minimal pipeline configuration can be used: pipeline_configuration.groovy libraries { npm { source_build { stageName = \"NPM Source Build\" script = \"build\" } unit_test { stageName = \"NPM Unit Tests\" script = \"test\" } } }","title":"Migrating from SDP 3.2 to 4.0"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/","text":"OpenShift \u00b6 OpenShift is Red Hat's enterprise Kubernetes distribution. This library allows you to perform deployments to static or ephemeral application environments with Helm . Steps \u00b6 Step Description deploy_to() Performs a deployment using Helm ephemeral(Closure body, ApplicationEnvironment) Creates a short-lived application environment for testing Overview \u00b6 Configuration \u00b6 The configurations for the OpenShift library can be specified at multiple levels. Given the additional layers of configuration the typical table of options would be less clear than examples, so it has been broken down per configuration portion. OpenShift Location \u00b6 The OpenShift location can be defined in the library spec or on a per application environment basis. For example, it's common to have a cluster for lower environments with a separate cluster for production. You would specify this as follows: application_environments { dev { short_name = \"dev\" long_name = \"Development\" } test { short_name = \"test\" long_name = \"Test\" } prod { short_name = \"prod\" long_name = \"Production\" openshift_url = \"https://openshift.prod.example.com:8443\" } } libraries { openshift { url = \"https://openshift.dev.example.com:8443\" } } With this configuration, https://openshift.dev.example.com:8443 would be used when deploying to dev and test while https://openshift.prod.example.com:8443 would be used when deploying to prod . Helm Configuration \u00b6 Helm is used as the deployment mechanism to OpenShift. Helm is a package manager and templating engine for Kubernetes manifests. Using Helm, the typical YAML manifests used to deploy to Kubernetes distributions can be templated for reuse. In this case, a different values file is used for each static application environment. Deploy the Tiller Server \u00b6 Instead of using Helm as a package manager by bundling the charts and deploying them to a chart repository, a configuration repository is used as the infrastructure as code mechanism. Create Helm Configuration Repository \u00b6 You'll need to create a GitHub repository to store the Helm chart for your application(s). See the Helm docs on provisioning a new chart for how to initialize the repository with the skeleton for your chart. How you choose to build your Helm chart is up to you. You can put every API object in the templates directory or have subcharts for each individual microservice. All the library does is clone the GitHub repository and deploy the chart using the specified values file. For most users, the only branch of the Helm configuration repository needed is the master branch. However, if you want to use different branches for different application environments, you can add a helm_chart_branch setting to your application environments in your pipeline config. Values File Conventions \u00b6 As container images are tagged using the Git SHA, SDP will clone your Helm configuration repository and update a key corresponding to the current version of each container image for each application environment. As such, a certain syntax is required in your values file. You must have a repos global key. SDP will automatically add elements for each repository to repos (assuming repos is a List already) and set their value to include the appropriate Git SHA. Note Since YAML keys can't have hyphens or numbers, any hyphens in repository names will be replaced with underscores and numbers will be spelled out. global : repos : - name : my_sample_application sha : abcdefgh - name : my_sample_application_two sha : abcdef - name : third_sample_application sha : a1b2c3 You can add whatever other keys are necessary to appropriately parameterize your Helm chart. Helm Configurations for the Library \u00b6 The Helm configuration repository, GitHub credential, Tiller namespace, and Tiller credential can be configured globally in the library spec and overridden for specific application environments. The values file used will default to values.${app_env.short_name}.yaml , but a different file can be selected through app_env.chart_values_file . The name of the release will default to app_env.short_name , but can be set through app_env.tiller_release_name . An example of Helm configurations: application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" tiller_release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" tiller_namespace = \"rhs-tiller-prod\" tiller_credential = \"rhs-tiller-prod\" } } libraries { openshift { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" } } Promoting Images \u00b6 It's often beneficial to build a container image once, and then promote that image through different application environments. This makes it possible to test the content of an image once in a lower environment, and remain confident that the results of those tests would be the same as an image is promoted. Promoting images also speeds up the CI/CD pipeline, as building a container image is often the most time-consuming part of the pipeline. By default, the deploy_to() step of the OpenShift pipeline library will promote a container image if it can expect one to exist, which is when the most recent code change was a merge into the given code branch. The image would be expected to be built from an earlier commit, or while there was an open PR. You can override this default for the entire pipeline by setting the promote_previous_image config setting to false . You can also choose whether to promote images for each application environment individually through the promote_previous_image application_environment setting. This application_environment setting takes priority over the config setting. An example of these settings' usage: application_environments { dev { short_name = \"dev\" long_name = \"Development\" promote_previous_image = false } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { openshift { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" promote_previous_image = true // (1) } } note: making this setting true is redundant, since true is the default Putting It All Together \u00b6 OpenShift Library Configuration Options Field Description Default Value Defined On Required openshift_url The OpenShift Console address when specified per application environment [app_env] if url isn't defined url The OpenShift Console address when specified globally library spec if openshift_url isn't defined helm_configuration_repository The GitHub Repository containing the helm chart(s) for this application both Yes helm_configuration_repository_credential The Jenkins credential ID to access the helm configuration GitHub repository both Yes tiller_namespace The tiller namespace for this application both Yes tiller_credential The Jenkins credential ID referencing an OpenShift credential both Yes tiller_release_name The name of the release to deploy application environment if [app_env].short_name isn't defined chart_values_file The values file to use for the release [app_env] if [app_env].short_name isn't defined helm_chart_branch The branch of helm_configuration_repository to use master [app_env] No promote_previous_image Whether to promote a previously built image (Boolean) true both No application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" tiller_release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" tiller_namespace = \"rhs-tiller-prod\" tiller_credential = \"rhs-tiller-prod\" openshift_url = \"https://openshift.prod.example.com:8443\" promote_previous_image = true } } libraries { openshift { url = \"https://openshift.dev.example.com:8443\" helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" promote_previous_image = false } } External Dependencies \u00b6 OpenShift is deployed and accessible from Jenkins The helm configuration repository defines the application as it would be deployed to OpenShift Values files follow the convention for repository names & Git SHAs The values file has the key globals.repo That key is a list of maps, each with two keys: name : the name of the source GitHub repository sha : the Git SHA for the last commit These maps are added automatically so long as global.repos is a list of maps A Jenkins credential exists to access the helm configuration repository A Jenkins credential exists to log in with OpenShift CLI The pipeline-utility-steps plugin is installed on Jenkins (supplies the readYAML step) Troubleshooting \u00b6 Updates were rejected \u00b6 Message : Updates were rejected because the remote contains work that you don't have locally. This is usually caused by another repository pushing to the same ref. You may want to first integrate the remote changes (for example: git pull ... ) before pushing again. Solution : Re-run the pipeline while no other pipeline jobs are running that would deploy to OpenShift. Explanation : After deploying to Helm, the pipeline attempts to update the helm-configuration-repository with the latest Git SHA for the pipeline's source code repository. However, if in the time between checking out the Helm chart from Git and pushing updates, another pipeline pushes its own updates, then Git will throw an error. Frequently Asked Questions \u00b6 Is there a way to securely deploy OpenShift Secrets to an ephemeral environment? Any secrets with the label ephemeral = true will be exported from the reference environment (for example: dev ) to the new ephemeral environment. Is GitHub required for the Helm configuration repository? This library was developed and tested using GitHub to manage the Helm configuration repository, but any Git SCM solution should work as well.","title":"OpenShift"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#openshift","text":"OpenShift is Red Hat's enterprise Kubernetes distribution. This library allows you to perform deployments to static or ephemeral application environments with Helm .","title":"OpenShift"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#steps","text":"Step Description deploy_to() Performs a deployment using Helm ephemeral(Closure body, ApplicationEnvironment) Creates a short-lived application environment for testing","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#overview","text":"","title":"Overview"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#configuration","text":"The configurations for the OpenShift library can be specified at multiple levels. Given the additional layers of configuration the typical table of options would be less clear than examples, so it has been broken down per configuration portion.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#openshift-location","text":"The OpenShift location can be defined in the library spec or on a per application environment basis. For example, it's common to have a cluster for lower environments with a separate cluster for production. You would specify this as follows: application_environments { dev { short_name = \"dev\" long_name = \"Development\" } test { short_name = \"test\" long_name = \"Test\" } prod { short_name = \"prod\" long_name = \"Production\" openshift_url = \"https://openshift.prod.example.com:8443\" } } libraries { openshift { url = \"https://openshift.dev.example.com:8443\" } } With this configuration, https://openshift.dev.example.com:8443 would be used when deploying to dev and test while https://openshift.prod.example.com:8443 would be used when deploying to prod .","title":"OpenShift Location"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#helm-configuration","text":"Helm is used as the deployment mechanism to OpenShift. Helm is a package manager and templating engine for Kubernetes manifests. Using Helm, the typical YAML manifests used to deploy to Kubernetes distributions can be templated for reuse. In this case, a different values file is used for each static application environment.","title":"Helm Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#deploy-the-tiller-server","text":"Instead of using Helm as a package manager by bundling the charts and deploying them to a chart repository, a configuration repository is used as the infrastructure as code mechanism.","title":"Deploy the Tiller Server"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#create-helm-configuration-repository","text":"You'll need to create a GitHub repository to store the Helm chart for your application(s). See the Helm docs on provisioning a new chart for how to initialize the repository with the skeleton for your chart. How you choose to build your Helm chart is up to you. You can put every API object in the templates directory or have subcharts for each individual microservice. All the library does is clone the GitHub repository and deploy the chart using the specified values file. For most users, the only branch of the Helm configuration repository needed is the master branch. However, if you want to use different branches for different application environments, you can add a helm_chart_branch setting to your application environments in your pipeline config.","title":"Create Helm Configuration Repository"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#values-file-conventions","text":"As container images are tagged using the Git SHA, SDP will clone your Helm configuration repository and update a key corresponding to the current version of each container image for each application environment. As such, a certain syntax is required in your values file. You must have a repos global key. SDP will automatically add elements for each repository to repos (assuming repos is a List already) and set their value to include the appropriate Git SHA. Note Since YAML keys can't have hyphens or numbers, any hyphens in repository names will be replaced with underscores and numbers will be spelled out. global : repos : - name : my_sample_application sha : abcdefgh - name : my_sample_application_two sha : abcdef - name : third_sample_application sha : a1b2c3 You can add whatever other keys are necessary to appropriately parameterize your Helm chart.","title":"Values File Conventions"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#helm-configurations-for-the-library","text":"The Helm configuration repository, GitHub credential, Tiller namespace, and Tiller credential can be configured globally in the library spec and overridden for specific application environments. The values file used will default to values.${app_env.short_name}.yaml , but a different file can be selected through app_env.chart_values_file . The name of the release will default to app_env.short_name , but can be set through app_env.tiller_release_name . An example of Helm configurations: application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" tiller_release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" tiller_namespace = \"rhs-tiller-prod\" tiller_credential = \"rhs-tiller-prod\" } } libraries { openshift { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" } }","title":"Helm Configurations for the Library"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#promoting-images","text":"It's often beneficial to build a container image once, and then promote that image through different application environments. This makes it possible to test the content of an image once in a lower environment, and remain confident that the results of those tests would be the same as an image is promoted. Promoting images also speeds up the CI/CD pipeline, as building a container image is often the most time-consuming part of the pipeline. By default, the deploy_to() step of the OpenShift pipeline library will promote a container image if it can expect one to exist, which is when the most recent code change was a merge into the given code branch. The image would be expected to be built from an earlier commit, or while there was an open PR. You can override this default for the entire pipeline by setting the promote_previous_image config setting to false . You can also choose whether to promote images for each application environment individually through the promote_previous_image application_environment setting. This application_environment setting takes priority over the config setting. An example of these settings' usage: application_environments { dev { short_name = \"dev\" long_name = \"Development\" promote_previous_image = false } prod { short_name = \"prod\" long_name = \"Production\" } } libraries { openshift { helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" promote_previous_image = true // (1) } } note: making this setting true is redundant, since true is the default","title":"Promoting Images"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#putting-it-all-together","text":"OpenShift Library Configuration Options Field Description Default Value Defined On Required openshift_url The OpenShift Console address when specified per application environment [app_env] if url isn't defined url The OpenShift Console address when specified globally library spec if openshift_url isn't defined helm_configuration_repository The GitHub Repository containing the helm chart(s) for this application both Yes helm_configuration_repository_credential The Jenkins credential ID to access the helm configuration GitHub repository both Yes tiller_namespace The tiller namespace for this application both Yes tiller_credential The Jenkins credential ID referencing an OpenShift credential both Yes tiller_release_name The name of the release to deploy application environment if [app_env].short_name isn't defined chart_values_file The values file to use for the release [app_env] if [app_env].short_name isn't defined helm_chart_branch The branch of helm_configuration_repository to use master [app_env] No promote_previous_image Whether to promote a previously built image (Boolean) true both No application_environments { dev { short_name = \"dev\" long_name = \"Development\" chart_values_file = \"dev_values.yaml\" } test { short_name = \"test\" long_name = \"Test\" tiller_release_name = \"banana\" } prod { short_name = \"prod\" long_name = \"Production\" tiller_namespace = \"rhs-tiller-prod\" tiller_credential = \"rhs-tiller-prod\" openshift_url = \"https://openshift.prod.example.com:8443\" promote_previous_image = true } } libraries { openshift { url = \"https://openshift.dev.example.com:8443\" helm_configuration_repository = \"https://github.boozallencsn.com/Red-Hat-Summit/helm-configuration.git\" helm_configuration_repository_credential = \"github\" tiller_namespace = \"rhs-tiller\" tiller_credential = \"rhs-tiller\" promote_previous_image = false } }","title":"Putting It All Together"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#external-dependencies","text":"OpenShift is deployed and accessible from Jenkins The helm configuration repository defines the application as it would be deployed to OpenShift Values files follow the convention for repository names & Git SHAs The values file has the key globals.repo That key is a list of maps, each with two keys: name : the name of the source GitHub repository sha : the Git SHA for the last commit These maps are added automatically so long as global.repos is a list of maps A Jenkins credential exists to access the helm configuration repository A Jenkins credential exists to log in with OpenShift CLI The pipeline-utility-steps plugin is installed on Jenkins (supplies the readYAML step)","title":"External Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#updates-were-rejected","text":"Message : Updates were rejected because the remote contains work that you don't have locally. This is usually caused by another repository pushing to the same ref. You may want to first integrate the remote changes (for example: git pull ... ) before pushing again. Solution : Re-run the pipeline while no other pipeline jobs are running that would deploy to OpenShift. Explanation : After deploying to Helm, the pipeline attempts to update the helm-configuration-repository with the latest Git SHA for the pipeline's source code repository. However, if in the time between checking out the Helm chart from Git and pushing updates, another pipeline pushes its own updates, then Git will throw an error.","title":"Updates were rejected"},{"location":"libraries/SDP%20Pipeline%20Libraries/openshift/#frequently-asked-questions","text":"Is there a way to securely deploy OpenShift Secrets to an ephemeral environment? Any secrets with the label ephemeral = true will be exported from the reference environment (for example: dev ) to the new ephemeral environment. Is GitHub required for the Helm configuration repository? This library was developed and tested using GitHub to manage the Helm configuration repository, but any Git SCM solution should work as well.","title":"Frequently Asked Questions"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/","text":"OWASP Dependency Check \u00b6 The OWASP Dependency Check library will use the namesake tool to scan a project's source code to identify components with known vulnerabilities. Official Website Documentation Steps \u00b6 Step Description application_dependency_scan() Uses the OWASP Dependency Check CLI to perform an application dependency scan Configuration \u00b6 OWASP Dependency Check Library Configuration Options Field Description Default Value scan ArrayList of Ant style paths to scan [ '.' ] exclude ArrayList of Ant style paths to exclude [ ] cvss_threshold A number between 0 and 10, inclusive, representing the failure threshold for vulnerabilities ( note: will never fail unless a threshold is provided) allow_suppression_file Allows whitelisting vulnerabilities using a suppression XML file true suppression_file Path to the suppression file (see here for how to create a suppression file) dependency-check-suppression.xml image_tag The tag for the scanner Docker image used 7.3.0-8.6-2 Example Configuration Snippet \u00b6 libraries { owasp_dep_check { scan = [ \"src\" ] cvss_threshold = 9 } } Viewing The Reports \u00b6 The application_dependency_scan step archives artifacts in multiple formats: HTML, JSON, JUnit XML, and CSV. CVSS Threshold & Scores \u00b6 From the Wikipedia article , The Common Vulnerability Scoring System (CVSS) is a free and open industry standard for assessing the severity of computer system security vulnerabilities. Scores range from 0 to 10, with 10 being the most severe. The pipeline can fail if a vulnerability is detected at or above a given threshold. This threshold is set with the cvss_threshold configuration option. For example, if cvss_threshold is set to 7, and a vulnerability with a CVSS score of 7.5 is detected, the pipeline will fail. If the vulnerability remains, but the cvss_threshold is set to 9, the pipeline will pass the OWASP Dependency Check scan. If you wish for the scan to pass regardless of the CVSS scores of detected vulnerabilities, don't set the cvss_threshold option.","title":"OWASP Dependency Check"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#owasp-dependency-check","text":"The OWASP Dependency Check library will use the namesake tool to scan a project's source code to identify components with known vulnerabilities. Official Website Documentation","title":"OWASP Dependency Check"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#steps","text":"Step Description application_dependency_scan() Uses the OWASP Dependency Check CLI to perform an application dependency scan","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#configuration","text":"OWASP Dependency Check Library Configuration Options Field Description Default Value scan ArrayList of Ant style paths to scan [ '.' ] exclude ArrayList of Ant style paths to exclude [ ] cvss_threshold A number between 0 and 10, inclusive, representing the failure threshold for vulnerabilities ( note: will never fail unless a threshold is provided) allow_suppression_file Allows whitelisting vulnerabilities using a suppression XML file true suppression_file Path to the suppression file (see here for how to create a suppression file) dependency-check-suppression.xml image_tag The tag for the scanner Docker image used 7.3.0-8.6-2","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#example-configuration-snippet","text":"libraries { owasp_dep_check { scan = [ \"src\" ] cvss_threshold = 9 } }","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#viewing-the-reports","text":"The application_dependency_scan step archives artifacts in multiple formats: HTML, JSON, JUnit XML, and CSV.","title":"Viewing The Reports"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_dep_check/#cvss-threshold-scores","text":"From the Wikipedia article , The Common Vulnerability Scoring System (CVSS) is a free and open industry standard for assessing the severity of computer system security vulnerabilities. Scores range from 0 to 10, with 10 being the most severe. The pipeline can fail if a vulnerability is detected at or above a given threshold. This threshold is set with the cvss_threshold configuration option. For example, if cvss_threshold is set to 7, and a vulnerability with a CVSS score of 7.5 is detected, the pipeline will fail. If the vulnerability remains, but the cvss_threshold is set to 9, the pipeline will pass the OWASP Dependency Check scan. If you wish for the scan to pass regardless of the CVSS scores of detected vulnerabilities, don't set the cvss_threshold option.","title":"CVSS Threshold &amp; Scores"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/","text":"OWASP ZAP \u00b6 OWASP Zed Attack Proxy (ZAP) is a tool that can help you automatically find security vulnerabilities in your web applications while you are developing and testing your applications. It's also a great tool for experienced penetration-testers to use for manual security testing. Steps \u00b6 Step Description penetration_test() Uses the OWASP ZAP CLI to perform penetration testing against the configured web application Configuration \u00b6 OWASP ZAP Library Configuration Options Field Description Default Value Options target The target web application address to test vulnerability_threshold Minimum alert level to include in report High one of Ignore , Low , Medium , High , or Informational target is set to env.FRONTEND_URL if available. If not then it uses the provided target . If neither is provided, an error is thrown. Example Configuration Snippet \u00b6 libraries { owasp_zap { target = \"https://example.com\" vulnerability_threshold = \"Low\" } } Results \u00b6","title":"OWASP ZAP"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/#owasp-zap","text":"OWASP Zed Attack Proxy (ZAP) is a tool that can help you automatically find security vulnerabilities in your web applications while you are developing and testing your applications. It's also a great tool for experienced penetration-testers to use for manual security testing.","title":"OWASP ZAP"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/#steps","text":"Step Description penetration_test() Uses the OWASP ZAP CLI to perform penetration testing against the configured web application","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/#configuration","text":"OWASP ZAP Library Configuration Options Field Description Default Value Options target The target web application address to test vulnerability_threshold Minimum alert level to include in report High one of Ignore , Low , Medium , High , or Informational target is set to env.FRONTEND_URL if available. If not then it uses the provided target . If neither is provided, an error is thrown.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/#example-configuration-snippet","text":"libraries { owasp_zap { target = \"https://example.com\" vulnerability_threshold = \"Low\" } }","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/owasp_zap/#results","text":"","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/protractor/","text":"Protractor \u00b6 Protractor is a test framework built for Angular and AngularJS applications that's used for end-to-end testing. The framework simulates user activity using the web application by running a developer's tests on a real browser. It adds a layer of tests to help ensure that newly added front-end code doesn't break already existing functionality or the build itself. Steps \u00b6 Step Description functional_test() leverages Protractor CLI to perform configured Protractor tests Configuration \u00b6 Library Configuration Options Field Description Default Value url Address of the website that will be tested enforce Boolean value that determines if a build will fail if a Protractor test fails config_file Name of the file where the Protractor configurations are set Example Configuration Snippet \u00b6 libraries { protractor { url = \"http://frontend-website.com\" enforce = true config_file = \"protractor.conf.js\" } }","title":"Protractor"},{"location":"libraries/SDP%20Pipeline%20Libraries/protractor/#protractor","text":"Protractor is a test framework built for Angular and AngularJS applications that's used for end-to-end testing. The framework simulates user activity using the web application by running a developer's tests on a real browser. It adds a layer of tests to help ensure that newly added front-end code doesn't break already existing functionality or the build itself.","title":"Protractor"},{"location":"libraries/SDP%20Pipeline%20Libraries/protractor/#steps","text":"Step Description functional_test() leverages Protractor CLI to perform configured Protractor tests","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/protractor/#configuration","text":"Library Configuration Options Field Description Default Value url Address of the website that will be tested enforce Boolean value that determines if a build will fail if a Protractor test fails config_file Name of the file where the Protractor configurations are set","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/protractor/#example-configuration-snippet","text":"libraries { protractor { url = \"http://frontend-website.com\" enforce = true config_file = \"protractor.conf.js\" } }","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/","text":"PyTest \u00b6 This library will execute Python unit tests leveraging the PyTest framework. Steps \u00b6 Step Description unit_test() executes unit tests via PyTest Configuration \u00b6 Configuration Options Field Description Required Default Value enforce_success Set to false if failing tests shouldn't fail the build No true requirements_file Relative path within the repository pointing to a Python requirements file No requirements.txt libraries { pytest { requirements_file = \"path/to/my/requirements.txt\" } } Results \u00b6 View an example of the HTML output that's been saved as a PDF here . Dependencies \u00b6","title":"PyTest"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/#pytest","text":"This library will execute Python unit tests leveraging the PyTest framework.","title":"PyTest"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/#steps","text":"Step Description unit_test() executes unit tests via PyTest","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/#configuration","text":"Configuration Options Field Description Required Default Value enforce_success Set to false if failing tests shouldn't fail the build No true requirements_file Relative path within the repository pointing to a Python requirements file No requirements.txt libraries { pytest { requirements_file = \"path/to/my/requirements.txt\" } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/#results","text":"View an example of the HTML output that's been saved as a PDF here .","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/pytest/#dependencies","text":"","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/","text":"SDP \u00b6 The SDP library provides helper steps used by multiple libraries within sdp-libraries. Steps \u00b6 Step Description inside_sdp_images(String image, Closure body) helper function that wraps docker.image(<image>).inside{} to execute a portion of the pipeline inside the specified container image runtime environment jteVersion a multi-method step that provides utilities for determining the current JTE version Lifecycle Hooks Step Hook Purpose archive_pipeline_config() @Init Writes the aggregated pipeline configuration to a file and saves it as a build artifact create_workspace_stash() @Validate If the pipeline job is a Multibranch Project, checkout the source code. In either case, save a stash called workspace for other libraries to consume. Configuration \u00b6 SDP Library Configuration Options Field Description Default Value images.registry This sets the registry the SDP library expects to find its Docker images images.repository The first path component in the repository name. For example if your images follow the format my-registry.com/sdp/* , this would be sdp sdp.images.cred Credentials used for the repository where different docker pipeline tools are stored sdp.images.docker_args Arguments to use when starting the container. Uses the same flags as docker run Important Unlike the Docker Library, the value in registry does include the protocol ( http:// https:// ) Example Configuration Snippet \u00b6 libraries { sdp { images { registry = \"https://docker.pkg.github.com\" // (1) repository = \"boozallen/sdp-images\" // (2) cred = \"github\" // (3) } } } the container registry that holds the SDP container images the container image repository that holds the SDP container images A Jenkins credential ID to authenticate to the container registry jteVersion \u00b6 jteVersion is a multi-method step that provides utilities for determining the current JTE version. This is particularly useful when making changes to support backwards compatibility. Methods | Method | Description | | jteVersion.get() | returns the current JTE version | | jteVersion.lessThan(String version) | returns true if the current JTE version is less than the parameter | | jteVersion.lessThanOrEqualTo(String version) | returns true if the current JTE version is less than or equal to the parameter | | jteVersion.greaterThan(String version) | returns true if the current JTE version is greater than the parameter | | jteVersion.greaterThanOrEqualTo(String version) | returns true if the current JTE version is greater than or equal to the parameter | | jteVersion.equalTo(String version) | returns true if the current JTE version is equal to the parameter | For example, if ( jteVersion . lessThan ( \"2.1\" )){ // code to run if current installed version is < 2.1 } else { // code to run if current installed version is >= 2.1 } Dependencies \u00b6 A Docker registry must be setup and configured. Credentials to the registry are also needed. A repository for the image being used by the given library is expected to be in the given registry. The repository name for the pipeline tools' images should be in the format \"${images.registry}/${images.repository}/tool-name\" . The Pipeline Utility Steps plugin is required.","title":"SDP"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#sdp","text":"The SDP library provides helper steps used by multiple libraries within sdp-libraries.","title":"SDP"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#steps","text":"Step Description inside_sdp_images(String image, Closure body) helper function that wraps docker.image(<image>).inside{} to execute a portion of the pipeline inside the specified container image runtime environment jteVersion a multi-method step that provides utilities for determining the current JTE version Lifecycle Hooks Step Hook Purpose archive_pipeline_config() @Init Writes the aggregated pipeline configuration to a file and saves it as a build artifact create_workspace_stash() @Validate If the pipeline job is a Multibranch Project, checkout the source code. In either case, save a stash called workspace for other libraries to consume.","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#configuration","text":"SDP Library Configuration Options Field Description Default Value images.registry This sets the registry the SDP library expects to find its Docker images images.repository The first path component in the repository name. For example if your images follow the format my-registry.com/sdp/* , this would be sdp sdp.images.cred Credentials used for the repository where different docker pipeline tools are stored sdp.images.docker_args Arguments to use when starting the container. Uses the same flags as docker run Important Unlike the Docker Library, the value in registry does include the protocol ( http:// https:// )","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#example-configuration-snippet","text":"libraries { sdp { images { registry = \"https://docker.pkg.github.com\" // (1) repository = \"boozallen/sdp-images\" // (2) cred = \"github\" // (3) } } } the container registry that holds the SDP container images the container image repository that holds the SDP container images A Jenkins credential ID to authenticate to the container registry","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#jteversion","text":"jteVersion is a multi-method step that provides utilities for determining the current JTE version. This is particularly useful when making changes to support backwards compatibility. Methods | Method | Description | | jteVersion.get() | returns the current JTE version | | jteVersion.lessThan(String version) | returns true if the current JTE version is less than the parameter | | jteVersion.lessThanOrEqualTo(String version) | returns true if the current JTE version is less than or equal to the parameter | | jteVersion.greaterThan(String version) | returns true if the current JTE version is greater than the parameter | | jteVersion.greaterThanOrEqualTo(String version) | returns true if the current JTE version is greater than or equal to the parameter | | jteVersion.equalTo(String version) | returns true if the current JTE version is equal to the parameter | For example, if ( jteVersion . lessThan ( \"2.1\" )){ // code to run if current installed version is < 2.1 } else { // code to run if current installed version is >= 2.1 }","title":"jteVersion"},{"location":"libraries/SDP%20Pipeline%20Libraries/sdp/#dependencies","text":"A Docker registry must be setup and configured. Credentials to the registry are also needed. A repository for the image being used by the given library is expected to be in the given registry. The repository name for the pipeline tools' images should be in the format \"${images.registry}/${images.repository}/tool-name\" . The Pipeline Utility Steps plugin is required.","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/slack/","text":"Slack \u00b6 Slack is a collection of tools and services that helps teams collaborate and work more efficiently. Services that it provides include a messaging platform as well as the capability to integrate with other 3rd party tools such as Trello, Jira, Splunk, Jenkins, and more. Steps \u00b6 slack() Dependencies \u00b6 More information can be found on the Slack plugin website .","title":"Slack"},{"location":"libraries/SDP%20Pipeline%20Libraries/slack/#slack","text":"Slack is a collection of tools and services that helps teams collaborate and work more efficiently. Services that it provides include a messaging platform as well as the capability to integrate with other 3rd party tools such as Trello, Jira, Splunk, Jenkins, and more.","title":"Slack"},{"location":"libraries/SDP%20Pipeline%20Libraries/slack/#steps","text":"slack()","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/slack/#dependencies","text":"More information can be found on the Slack plugin website .","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/","text":"SonarQube \u00b6 SonarQube is a tool used for static code analysis . Static code analysis is validating code as-written against industry standard practices. It will help you find best practice violations and potential security vulnerabilities. Organizations can define Quality Profiles which are custom rule profiles that projects must use. Quality Gates are then rules defining the organizational policies for code quality. SDP will, by default, fail the build if the Quality Gate fails. Steps \u00b6 Step Description static_code_analysis() Leverages the sonar-scanner CLI to perform static code analysis and sends results to the configured SonarQube server Configuration \u00b6 SonarQube Library Configuration Options Field Description Default Value installation_name The name of the SonarQube installation configured in Manage Jenkins > Configure System \"SonarQube\" credential_id The Jenkins credential ID to use when authenticating to SonarQube. Can either be a valid username/password or an API Token stored in a Secret Text credential type. If unset, the library will check if the installation defined via installation_name has a server authorization token configured. If a server authorization token has been provided in the plugin configuration, then that will be the default. If unset, then a credential id of \"sonarqube\" will be assumed. wait_for_quality_gate Whether to wait for SonarQube to send a webhook back to Jenkins notifying with the Quality Gate result true enforce_quality_gate Determine whether the build will fail if the code doesn't pass the quality gate true stage_display_name Purely aesthetic. The name of the stage block during analysis for pipeline visualization in the Jenkins console. \"SonarQube Analysis\" timeout_duration The number representing how long to wait for the Quality Gate response before timing out 1 timeout_unit One of [ \"NANOSECONDS\", \"MICROSECONDS\", \"MILLISECONDS\", \"SECONDS\", \"MINUTES\", \"HOURS\", \"DAYS\" ] \"HOURS\" cli_parameters a list of additional CLI analysis parameters to pass the sonar-scanner CLI [ ] unstash a list of pre-existing stashes to try to unstash. Useful if a previous step creates compiled classes or test results for SonarQube to inspect. [ ] Analysis Parameters \u00b6 In SonarQube, project analysis settings can be provided to the SonarScanner CLI in multiple ways. The SonarScanner will look for the presence of a sonar-project.properties file in the current working directory. Alternatively, users can use this library's cli_parameters configuration to pass an array of CLI analysis parameters to SonarScanner. For example, libraries { sonarqube { cli_parameters = [ \"-Dsonar.projectKey=myCoolProject\" , \"-Dsonar.projectName='My Cool Project'\" ] } } Environment Variables \u00b6 It's possible to use pipeline environment variables to populate the analysis parameters. This is especially useful when used with one of the source code management libraries to reference the branch name. Configuration File \u00b6 .sonar-project.properties sonar.projectName=My Cool Project: ${env.BRANCH_NAME} Parameters \u00b6 .pipeline_config.groovy libraries { sonarqube { cli_parameters = [ \"-Dsonar.projectName=\\\"My Cool Project: \\$BRANCH_NAME\\\"\" ] } } Dependencies \u00b6 A SonarQube server should be deployed The SonarQube Scanner plugin should be installed The SonarQube Installation must be configured in Manage Jenkins > Configure System > SonarQube servers The \"Enable injection of SonarQube server configuration as build environment variables\" checkbox should be checked Authentication \u00b6 This library supports both username/password and API Token authentication to SonarQube. If anonymous access is disabled for the SonarQube Server ( it probably should be ), then you will need to create an API Token and store it as a Secret Text credential in the Jenkins Credential Store for reference in Manage Jenkins > Configure System > Sonarqube servers as the Server authentication token .","title":"SonarQube"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#sonarqube","text":"SonarQube is a tool used for static code analysis . Static code analysis is validating code as-written against industry standard practices. It will help you find best practice violations and potential security vulnerabilities. Organizations can define Quality Profiles which are custom rule profiles that projects must use. Quality Gates are then rules defining the organizational policies for code quality. SDP will, by default, fail the build if the Quality Gate fails.","title":"SonarQube"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#steps","text":"Step Description static_code_analysis() Leverages the sonar-scanner CLI to perform static code analysis and sends results to the configured SonarQube server","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#configuration","text":"SonarQube Library Configuration Options Field Description Default Value installation_name The name of the SonarQube installation configured in Manage Jenkins > Configure System \"SonarQube\" credential_id The Jenkins credential ID to use when authenticating to SonarQube. Can either be a valid username/password or an API Token stored in a Secret Text credential type. If unset, the library will check if the installation defined via installation_name has a server authorization token configured. If a server authorization token has been provided in the plugin configuration, then that will be the default. If unset, then a credential id of \"sonarqube\" will be assumed. wait_for_quality_gate Whether to wait for SonarQube to send a webhook back to Jenkins notifying with the Quality Gate result true enforce_quality_gate Determine whether the build will fail if the code doesn't pass the quality gate true stage_display_name Purely aesthetic. The name of the stage block during analysis for pipeline visualization in the Jenkins console. \"SonarQube Analysis\" timeout_duration The number representing how long to wait for the Quality Gate response before timing out 1 timeout_unit One of [ \"NANOSECONDS\", \"MICROSECONDS\", \"MILLISECONDS\", \"SECONDS\", \"MINUTES\", \"HOURS\", \"DAYS\" ] \"HOURS\" cli_parameters a list of additional CLI analysis parameters to pass the sonar-scanner CLI [ ] unstash a list of pre-existing stashes to try to unstash. Useful if a previous step creates compiled classes or test results for SonarQube to inspect. [ ]","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#analysis-parameters","text":"In SonarQube, project analysis settings can be provided to the SonarScanner CLI in multiple ways. The SonarScanner will look for the presence of a sonar-project.properties file in the current working directory. Alternatively, users can use this library's cli_parameters configuration to pass an array of CLI analysis parameters to SonarScanner. For example, libraries { sonarqube { cli_parameters = [ \"-Dsonar.projectKey=myCoolProject\" , \"-Dsonar.projectName='My Cool Project'\" ] } }","title":"Analysis Parameters"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#environment-variables","text":"It's possible to use pipeline environment variables to populate the analysis parameters. This is especially useful when used with one of the source code management libraries to reference the branch name.","title":"Environment Variables"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#configuration-file","text":".sonar-project.properties sonar.projectName=My Cool Project: ${env.BRANCH_NAME}","title":"Configuration File"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#parameters","text":".pipeline_config.groovy libraries { sonarqube { cli_parameters = [ \"-Dsonar.projectName=\\\"My Cool Project: \\$BRANCH_NAME\\\"\" ] } }","title":"Parameters"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#dependencies","text":"A SonarQube server should be deployed The SonarQube Scanner plugin should be installed The SonarQube Installation must be configured in Manage Jenkins > Configure System > SonarQube servers The \"Enable injection of SonarQube server configuration as build environment variables\" checkbox should be checked","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/sonarqube/#authentication","text":"This library supports both username/password and API Token authentication to SonarQube. If anonymous access is disabled for the SonarQube Server ( it probably should be ), then you will need to create an API Token and store it as a Secret Text credential in the Jenkins Credential Store for reference in Manage Jenkins > Configure System > Sonarqube servers as the Server authentication token .","title":"Authentication"},{"location":"libraries/SDP%20Pipeline%20Libraries/syft/","text":"Syft \u00b6 This library allows you to generate a Software Bill of Materials (SBOM) for each container built in your project using the Syft tool . Steps \u00b6 Step Description generate_sbom() Generates and archives SBOM files in JSON format Configuration \u00b6 Library Configuration Description Type Default Value Options raw_results_file The base name of the report file generated. Omit Extension. String syft-sbom-results sbom_container Name of the container image containing the syft executable. String syft:0.47.0 sbom_format The valid formats a report can be generated in. ArrayList ['json'] ['json', 'text', 'cyclonedx-xml', 'cyclonedx-json', 'spdx-tag-value', 'spdx-json', 'github', 'table'] remove_syft_config Removes .syft.yaml from the workspace if needed. Boolean True True, False config_name Name of config to remove. String .syft.yaml pipeline_config.groovy libraries { syft { raw_results_file = \"syft-scan\" sbom_container = \"syft:v0.47.0\" sbom_format = [ 'json' , 'spdx-json' , 'table' ] remove_syft_config = true config_name = \".syft.yaml\" } } Dependencies \u00b6 Base SDP library Docker SDP library","title":"Syft"},{"location":"libraries/SDP%20Pipeline%20Libraries/syft/#syft","text":"This library allows you to generate a Software Bill of Materials (SBOM) for each container built in your project using the Syft tool .","title":"Syft"},{"location":"libraries/SDP%20Pipeline%20Libraries/syft/#steps","text":"Step Description generate_sbom() Generates and archives SBOM files in JSON format","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/syft/#configuration","text":"Library Configuration Description Type Default Value Options raw_results_file The base name of the report file generated. Omit Extension. String syft-sbom-results sbom_container Name of the container image containing the syft executable. String syft:0.47.0 sbom_format The valid formats a report can be generated in. ArrayList ['json'] ['json', 'text', 'cyclonedx-xml', 'cyclonedx-json', 'spdx-tag-value', 'spdx-json', 'github', 'table'] remove_syft_config Removes .syft.yaml from the workspace if needed. Boolean True True, False config_name Name of config to remove. String .syft.yaml pipeline_config.groovy libraries { syft { raw_results_file = \"syft-scan\" sbom_container = \"syft:v0.47.0\" sbom_format = [ 'json' , 'spdx-json' , 'table' ] remove_syft_config = true config_name = \".syft.yaml\" } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/syft/#dependencies","text":"Base SDP library Docker SDP library","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/","text":"Sysdig Secure \u00b6 This library leverages a script from Sysdig Secure ( inline scanning script ) to scan container images, report the information to the Sysdig Secure server, and download a PDF report of the findings. Steps \u00b6 Step Description scan_container_image() Scans container images determined by get_images_to_build() Configuration \u00b6 Configuration Options Field Type Description Default Value scan_script_url String An address from which to download the inline_scan.sh file https://download.sysdig.com/stable/inline_scan.sh sysdig_secure_url String The Sysdig Secure address to publish results to https://secure.sysdig.com cred String A string matching a credential id of a secret text credential in the Jenkins Credential store holding an API token to authenticate to the Sysdig Secure API enforce_success Boolean Whether to fail the build if the scan fails true libraries { sysdig_secure { cred = \"sysdig-secure-api-token\" } } Results \u00b6 The scan_container_images() step will generate a PDF report of the scan if the upload to the Sysdig Secure API is successful. Here's an example . Dependencies \u00b6 This library, by nature of the inline scanning script, requires that: a running docker daemon is available internet access to pull an image from Docker Hub Note At the time of writing, this library could be expanded to pass a custom image to perform the scanning, perhaps helpful if proxying through a local registry, by setting the environment variable SYSDIG_CI_IMAGE as part of the command invocation. Troubleshooting \u00b6","title":"Sysdig Secure"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#sysdig-secure","text":"This library leverages a script from Sysdig Secure ( inline scanning script ) to scan container images, report the information to the Sysdig Secure server, and download a PDF report of the findings.","title":"Sysdig Secure"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#steps","text":"Step Description scan_container_image() Scans container images determined by get_images_to_build()","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#configuration","text":"Configuration Options Field Type Description Default Value scan_script_url String An address from which to download the inline_scan.sh file https://download.sysdig.com/stable/inline_scan.sh sysdig_secure_url String The Sysdig Secure address to publish results to https://secure.sysdig.com cred String A string matching a credential id of a secret text credential in the Jenkins Credential store holding an API token to authenticate to the Sysdig Secure API enforce_success Boolean Whether to fail the build if the scan fails true libraries { sysdig_secure { cred = \"sysdig-secure-api-token\" } }","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#results","text":"The scan_container_images() step will generate a PDF report of the scan if the upload to the Sysdig Secure API is successful. Here's an example .","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#dependencies","text":"This library, by nature of the inline scanning script, requires that: a running docker daemon is available internet access to pull an image from Docker Hub Note At the time of writing, this library could be expanded to pass a custom image to perform the scanning, perhaps helpful if proxying through a local registry, by setting the environment variable SYSDIG_CI_IMAGE as part of the command invocation.","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/sysdig_secure/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/","text":"Terraform \u00b6 This library leverages Terraform to manage deployments of Infrastructure as Code to different environments. Steps \u00b6 Step Description deploy_to(application_environment) performs a terraform apply Configuration \u00b6 Working Directory \u00b6 The working directory from which to run Terraform commands can be specified on the application environment pass to deploy_to or within the library configuration. Pipeline Configuration application_environments { dev prod { terraform { working_directory = \"terraform-prod\" } } } libraries { terraform { working_directory = \"default-directory\" } } Pipeline Template /* because dev.terraform.working_directory is not set the library will fallback to the library's configuration and execute terraform commands within the \"default\" directory */ deploy_to dev /* because prod.terraform.working_directory is set to \"terraform-prod\" the terraform commands will be executed within ./terraform-prod */ deploy_to prod Note If the working directory isn't defined on either the library configuration or the application environment then the default value \".\" will be used. Secrets \u00b6 This library allows you to configure secrets as environment variables. This can be done in both the library configuration or application environments. There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored are in the Jenkins credential store. Library Secrets Syntax libraries { terraform { secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } The name of each credential block isn't important, and only used when describing configuration errors found by the step. To pass secrets on a per application environment basis, define a app_env.terraform.secrets block: Application Environments Secrets Syntax application_environments { prod { terraform { secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } Important If the same secret block is defined on both the application environment and the library configuration, the application environment secret definition will be used. Providers \u00b6 The SDP Terraform Container Image can bundle custom providers, if necessary. Sysdig Provider \u00b6 The Sysdig Terraform Provider is bundled with the Terraform image. To configure this provider, it is advisable to create secrets for SYSDIG_SECURE_API_TOKEN and SYSDIG_MONITOR_API_TOKEN . These environment variables can be consumed by the provider to configure the required secrets. Dependencies \u00b6 Troubleshooting \u00b6","title":"Terraform"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#terraform","text":"This library leverages Terraform to manage deployments of Infrastructure as Code to different environments.","title":"Terraform"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#steps","text":"Step Description deploy_to(application_environment) performs a terraform apply","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#configuration","text":"","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#working-directory","text":"The working directory from which to run Terraform commands can be specified on the application environment pass to deploy_to or within the library configuration. Pipeline Configuration application_environments { dev prod { terraform { working_directory = \"terraform-prod\" } } } libraries { terraform { working_directory = \"default-directory\" } } Pipeline Template /* because dev.terraform.working_directory is not set the library will fallback to the library's configuration and execute terraform commands within the \"default\" directory */ deploy_to dev /* because prod.terraform.working_directory is set to \"terraform-prod\" the terraform commands will be executed within ./terraform-prod */ deploy_to prod Note If the working directory isn't defined on either the library configuration or the application environment then the default value \".\" will be used.","title":"Working Directory"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#secrets","text":"This library allows you to configure secrets as environment variables. This can be done in both the library configuration or application environments. There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored are in the Jenkins credential store. Library Secrets Syntax libraries { terraform { secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } The name of each credential block isn't important, and only used when describing configuration errors found by the step. To pass secrets on a per application environment basis, define a app_env.terraform.secrets block: Application Environments Secrets Syntax application_environments { prod { terraform { secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } Important If the same secret block is defined on both the application environment and the library configuration, the application environment secret definition will be used.","title":"Secrets"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#providers","text":"The SDP Terraform Container Image can bundle custom providers, if necessary.","title":"Providers"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#sysdig-provider","text":"The Sysdig Terraform Provider is bundled with the Terraform image. To configure this provider, it is advisable to create secrets for SYSDIG_SECURE_API_TOKEN and SYSDIG_MONITOR_API_TOKEN . These environment variables can be consumed by the provider to configure the required secrets.","title":"Sysdig Provider"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#dependencies","text":"","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/terraform/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/","text":"Twistlock \u00b6 Twistlock is an automated and scalable container cyber-security platform. Twistlock manages a full-lifecycle vulnerability and compliance management to application-tailored runtime defense and cloud native firewalls. Twistlock helps secure your containers and modern applications against threats across the entire application lifecycle. SDP can integrate with Twistlock to perform container image scanning . Steps \u00b6 Step Description scan_container_image() Downloads the Twistlock CLI from the Twistlock Console and performs container image scanning Configuration \u00b6 Twistlock Library Configuration Options Field Description Default Value url The Twistlock Console address credential The Jenkins credential ID to access Twistlock Console Example Configuration Snippet \u00b6 libraries { twistlock { url = \"https://twistlock.apps.ocp.microcaas.net\" credential = \"twistlock\" } } Dependencies \u00b6 Twistlock is deployed and accessible from Jenkins A credential has been placed in the Jenkins credential store to access the console A separate container building library that implements get_images_to_build() Twistlock Scan Results \u00b6 Jenkins will output a text based table of the scan results. A more descriptive JSON file is archived that contains details of CVE and compliance vulnerabilities found during the scan. CVE Results: ----------------------------------------- Low: [0-9]* Number of Low vulnerabilities Medium: [0-9]* Number of Medium vulnerabilities High: [0-9]* Number of High vulnerabilities Critical: [0-9]* Number of Critical vulnerabilities Compliance Results: ----------------------------------------- Low: [0-9]* Number of Low compliance violations Medium: [0-9]* Number of Medium compliance violations High: [0-9]* Number of High compliance violations Critical: [0-9]* Number of Critical compliance violations","title":"Twistlock"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#twistlock","text":"Twistlock is an automated and scalable container cyber-security platform. Twistlock manages a full-lifecycle vulnerability and compliance management to application-tailored runtime defense and cloud native firewalls. Twistlock helps secure your containers and modern applications against threats across the entire application lifecycle. SDP can integrate with Twistlock to perform container image scanning .","title":"Twistlock"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#steps","text":"Step Description scan_container_image() Downloads the Twistlock CLI from the Twistlock Console and performs container image scanning","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#configuration","text":"Twistlock Library Configuration Options Field Description Default Value url The Twistlock Console address credential The Jenkins credential ID to access Twistlock Console","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#example-configuration-snippet","text":"libraries { twistlock { url = \"https://twistlock.apps.ocp.microcaas.net\" credential = \"twistlock\" } }","title":"Example Configuration Snippet"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#dependencies","text":"Twistlock is deployed and accessible from Jenkins A credential has been placed in the Jenkins credential store to access the console A separate container building library that implements get_images_to_build()","title":"Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/twistlock/#twistlock-scan-results","text":"Jenkins will output a text based table of the scan results. A more descriptive JSON file is archived that contains details of CVE and compliance vulnerabilities found during the scan. CVE Results: ----------------------------------------- Low: [0-9]* Number of Low vulnerabilities Medium: [0-9]* Number of Medium vulnerabilities High: [0-9]* Number of High vulnerabilities Critical: [0-9]* Number of Critical vulnerabilities Compliance Results: ----------------------------------------- Low: [0-9]* Number of Low compliance violations Medium: [0-9]* Number of Medium compliance violations High: [0-9]* Number of High compliance violations Critical: [0-9]* Number of Critical compliance violations","title":"Twistlock Scan Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/","text":"webhint \u00b6 webhint is a customizable linting tool that helps you improve your site's accessibility, speed, cross-browser compatibility, and more by checking your code for best practices and common errors. Steps \u00b6 Step Description accessibility_compliance_scan() generates website developer hints from the given web address Library Configuration Options \u00b6 Configuration Options Field Description Default Value url web address to analyze extender Array - Optional - Hint types. See the documentation for more information. [\"accessibility\"] failThreshold Optional - Hint limit at which the jenkins build will fail 25 warnThreshold Optional - Hint limit at which the jenkins build will issue a warning 10 libraries { url = \"your_url_here\" extender = [ \"progressive-web-apps\" ] failThreshold = 35 warnThreshold = 25 } Results \u00b6 View an example HTML report, saved to PDF . External Dependencies \u00b6 none Troubleshooting \u00b6","title":"webhint"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#webhint","text":"webhint is a customizable linting tool that helps you improve your site's accessibility, speed, cross-browser compatibility, and more by checking your code for best practices and common errors.","title":"webhint"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#steps","text":"Step Description accessibility_compliance_scan() generates website developer hints from the given web address","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#library-configuration-options","text":"Configuration Options Field Description Default Value url web address to analyze extender Array - Optional - Hint types. See the documentation for more information. [\"accessibility\"] failThreshold Optional - Hint limit at which the jenkins build will fail 25 warnThreshold Optional - Hint limit at which the jenkins build will issue a warning 10 libraries { url = \"your_url_here\" extender = [ \"progressive-web-apps\" ] failThreshold = 35 warnThreshold = 25 }","title":"Library Configuration Options"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#results","text":"View an example HTML report, saved to PDF .","title":"Results"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#external-dependencies","text":"none","title":"External Dependencies"},{"location":"libraries/SDP%20Pipeline%20Libraries/webhint/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/","text":"Yarn \u00b6 Run Yarn script commands in an NVM container with a specified Node version. Configuration \u00b6 All configs can be set in either the library config or the Application Environment. All configs set in Application Environment take precedence. Environment variables and secrets set in the library config are concatenated with those set in the Application Environment. Environment variables and secrets with the same key are set to the definition contained in the Application Environment. Steps \u00b6 Steps are configured dynamically in either the library config or the Application Environment. pipeline_configuration.groovy libraries { yarn { [ step_name ] { // config fields described below } ... } } Example Library Configuration \u00b6 Field Description Default nvm_container The container image to use nvm:1.0.0 node_version Node version to run Yarn within (installed via NVM) lts/* yarn_version Yarn version to use latest <step name>.stageName stage name displayed in the Jenkins dashboard N/A <step name>.script Yarn script ran by the step N/A <step name>.artifacts array of glob patterns for artifacts that should be archived <step name>.yarnInstall Yarn install command to run; Yarn install can be skipped with value skip ; Also supports immutable for Yarn 3 setups frozen-lockfile <step name>.env environment variables to make available to the Yarn process; can include key/value pairs and secrets [] <step name>.env.secrets text or username/password credentials to make available to the Yarn process; must be present and available in Jenkins credential store [] <step name>.useEslintPlugin if the Jenkins ESLint Plugin is installed, will run the recordIssues step to send lint results to the plugin dashboard false Full Configuration Example \u00b6 Each available method has config options that can be specified in the Application Environment or within the library configuration. pipeline_configuration.groovy application_environments { dev prod { yarn { node_version = \"14.16.1\" yarn_version = \"1.22.17\" unit_test { stageName = \"Yarn Unit Tests\" script = \"full-test-suite\" artifacts = [ \"coverage/lcov.info\" ] yarnInstall = \"frozen-lockfile\" env { someKey = \"prodValue for tests\" // (1) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } // (2) } } } source_build { stageName = \"Yarn Source Build\" script = \"prod-build\" env { someKey = \"prodValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } lint_code { stageName = \"Yarn Lint Code\" script = \"lint\" artifacts = [ \"eslint-report.json\" , \"eslint-report.html\" , \"eslint-report.xml\" , ] useEslintPlugin = true env { someKey = \"prodValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } } } libraries { yarn { node_version = \"lts/*\" yarn_version = \"latest\" unit_test { stageName = \"Yarn Unit Tests\" script = \"test\" yarnInstall = \"install\" env { someKey = \"someValue for tests\" // (3) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } // (4) } } } source_build { stageName = \"Yarn Source Build\" script = \"build\" yarnInstall = \"skip\" env { someKey = \"someValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } lint_code { stageName = \"Yarn Lint Code\" script = \"lint\" yarnInstall = \"skip\" env { someKey = \"someValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } } more envVars as needed more secrets as needed more envVars as needed more secrets as needed This example shows the prod Application Environment overriding configs set in the library config. source_build.yarnInstall is preserved as set in library config, since it isn't overridden by the Application Environment. Minimal Configuration Example \u00b6 The minimal configuration for this library is: pipeline_configuration.groovy libraries { yarn { unit_test { stageName = \"Yarn Unit Tests\" script = \"test\" } } } Secrets \u00b6 There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored in the Jenkins credential store and be available to the pipeline. The name of each credential block (such as someTextCredential ) is arbitrary. It's just a key, used to supersede library config with Application Environment configs, and when describing configuration errors found by the step. Dependencies \u00b6 The SDP library must be loaded inside the pipeline_config.groovy file.","title":"Yarn"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#yarn","text":"Run Yarn script commands in an NVM container with a specified Node version.","title":"Yarn"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#configuration","text":"All configs can be set in either the library config or the Application Environment. All configs set in Application Environment take precedence. Environment variables and secrets set in the library config are concatenated with those set in the Application Environment. Environment variables and secrets with the same key are set to the definition contained in the Application Environment.","title":"Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#steps","text":"Steps are configured dynamically in either the library config or the Application Environment. pipeline_configuration.groovy libraries { yarn { [ step_name ] { // config fields described below } ... } }","title":"Steps"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#example-library-configuration","text":"Field Description Default nvm_container The container image to use nvm:1.0.0 node_version Node version to run Yarn within (installed via NVM) lts/* yarn_version Yarn version to use latest <step name>.stageName stage name displayed in the Jenkins dashboard N/A <step name>.script Yarn script ran by the step N/A <step name>.artifacts array of glob patterns for artifacts that should be archived <step name>.yarnInstall Yarn install command to run; Yarn install can be skipped with value skip ; Also supports immutable for Yarn 3 setups frozen-lockfile <step name>.env environment variables to make available to the Yarn process; can include key/value pairs and secrets [] <step name>.env.secrets text or username/password credentials to make available to the Yarn process; must be present and available in Jenkins credential store [] <step name>.useEslintPlugin if the Jenkins ESLint Plugin is installed, will run the recordIssues step to send lint results to the plugin dashboard false","title":"Example Library Configuration"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#full-configuration-example","text":"Each available method has config options that can be specified in the Application Environment or within the library configuration. pipeline_configuration.groovy application_environments { dev prod { yarn { node_version = \"14.16.1\" yarn_version = \"1.22.17\" unit_test { stageName = \"Yarn Unit Tests\" script = \"full-test-suite\" artifacts = [ \"coverage/lcov.info\" ] yarnInstall = \"frozen-lockfile\" env { someKey = \"prodValue for tests\" // (1) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } // (2) } } } source_build { stageName = \"Yarn Source Build\" script = \"prod-build\" env { someKey = \"prodValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } lint_code { stageName = \"Yarn Lint Code\" script = \"lint\" artifacts = [ \"eslint-report.json\" , \"eslint-report.html\" , \"eslint-report.xml\" , ] useEslintPlugin = true env { someKey = \"prodValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"prod-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"prod-credential-id\" } } } } } } } libraries { yarn { node_version = \"lts/*\" yarn_version = \"latest\" unit_test { stageName = \"Yarn Unit Tests\" script = \"test\" yarnInstall = \"install\" env { someKey = \"someValue for tests\" // (3) secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } // (4) } } } source_build { stageName = \"Yarn Source Build\" script = \"build\" yarnInstall = \"skip\" env { someKey = \"someValue for builds\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } lint_code { stageName = \"Yarn Lint Code\" script = \"lint\" yarnInstall = \"skip\" env { someKey = \"someValue for linting\" secrets { someTextCredential { type = \"text\" name = \"VARIABLE_NAME\" id = \"some-credential-id\" } someUsernamePasswordCredential { type = \"usernamePassword\" usernameVar = \"USER\" passwordVar = \"PASS\" id = \"some-credential-id\" } } } } } } more envVars as needed more secrets as needed more envVars as needed more secrets as needed This example shows the prod Application Environment overriding configs set in the library config. source_build.yarnInstall is preserved as set in library config, since it isn't overridden by the Application Environment.","title":"Full Configuration Example"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#minimal-configuration-example","text":"The minimal configuration for this library is: pipeline_configuration.groovy libraries { yarn { unit_test { stageName = \"Yarn Unit Tests\" script = \"test\" } } }","title":"Minimal Configuration Example"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#secrets","text":"There are two types of secrets currently supported: secret text and username/password credentials. These credentials must be stored in the Jenkins credential store and be available to the pipeline. The name of each credential block (such as someTextCredential ) is arbitrary. It's just a key, used to supersede library config with Application Environment configs, and when describing configuration errors found by the step.","title":"Secrets"},{"location":"libraries/SDP%20Pipeline%20Libraries/yarn/#dependencies","text":"The SDP library must be loaded inside the pipeline_config.groovy file.","title":"Dependencies"},{"location":"tutorials/overview/","text":"Overview \u00b6 Tutorials are learning oriented lessons to teach users about this project. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! Tutorial Description","title":"Overview"},{"location":"tutorials/overview/#overview","text":"Tutorials are learning oriented lessons to teach users about this project. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! Tutorial Description","title":"Overview"}]}